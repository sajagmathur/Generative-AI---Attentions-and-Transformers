{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c839e53d-a7cb-4cce-9e1d-da35230f048f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "#we use pytorch when parameters require complex cmputations. Helps in neural networks\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1140481d-d371-4654-8621-54b7a52299de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Give an input data\n",
    "sentence = \"My name is Sajag mathur. He likes classical music.\"     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64145231-6cd3-44c5-a154-ffcf3bb04073",
   "metadata": {},
   "source": [
    "# Tokenize words and provide user defined values. We will use a user defined function for the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04d0a402-6cc4-4a6d-be87-5baa944a05c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([14891, 10332,   193, 13001,  3627,  3726,  7516,  7895, 19014])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize(text: str, vocab_size: int) -> torch.Tensor:\n",
    "    \"\"\"Dummy text tokenizer.\"\"\"\n",
    "    #break into chunks\n",
    "    words = text.split(\" \")\n",
    "    #give integer value to each chunk - this is for testing. In case you want consistant values, you replace by huggingface transformer and integer values\n",
    "    return torch.randint(0, vocab_size, [len(words)])\n",
    "\n",
    "#give a hyperparameter called vocab_size --> this means that total number of unique tokens that model can recognize and process is 20000.\n",
    "VOCAB_SIZE = 20000\n",
    "\n",
    "tokenized_sentence = tokenize(sentence, VOCAB_SIZE)\n",
    "n_tokens = len(tokenized_sentence)\n",
    "tokenized_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942d68b6-2280-4039-ada5-2e9b3ebdfb0c",
   "metadata": {},
   "source": [
    "# And embed each token into a vector space using PyTorch's [torch.nn.Embedding](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html#torch.nn.Embedding) module.\n",
    "\n",
    "\n",
    "This image shows how the embedding layer in a neural network (like a transformer) works:\n",
    "\n",
    "1. Input tokens (such as words or subwords) are represented as symbols (like “V”, “B”, etc.) on the right.\n",
    "\n",
    "2. Each token is mapped to a unique vector (a row of numbers), seen as colored circles and columns in the middle.\n",
    "\n",
    "3. The embedding layer transforms each token into its corresponding numerical vector (embedding), stacking these vectors together to form the input for the rest of the model.\n",
    "\n",
    "In short: The embedding layer turns text tokens into high-dimensional numeric representations that the model can understand and process further.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8afc00cf-2442-474b-98ca-a16023643ed8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9, 32])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Generate Embeddings and convert input tokens into vectors --> This is run through neural network.embedding\n",
    "# Vocabulary Size and Dimension of Embeddings --> you tokenize the sentence\n",
    "EMBEDDING_DIM = 32\n",
    "\n",
    "embedding_layer = nn.Embedding(VOCAB_SIZE, EMBEDDING_DIM)\n",
    "embedded_tokens = embedding_layer(tokenized_sentence)\n",
    "embedded_tokens.shape\n",
    "\n",
    "#Torch.size(size of input data, embedding dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7544c0f8-24af-4ae8-86aa-64680c942686",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.0857e+00,  1.0020e+00, -2.3250e-01,  1.4590e+00,  1.2314e+00,\n",
       "         -5.9862e-01, -1.2298e-01,  4.9957e-01, -4.1830e-01, -2.0499e+00,\n",
       "         -9.5629e-01,  5.8214e-01,  5.1003e-01,  7.7742e-01,  2.9274e-01,\n",
       "          1.2250e-01,  4.7286e-01,  3.8568e-01,  6.8314e-01, -9.0941e-02,\n",
       "         -1.5446e-01,  6.5623e-01,  1.2459e+00,  5.5247e-01,  8.9871e-01,\n",
       "          3.5293e-01,  1.5774e+00, -7.0908e-01,  2.2781e+00,  1.7482e-01,\n",
       "         -7.8116e-01, -3.4698e-01],\n",
       "        [-1.1850e+00, -3.2007e-01, -2.4116e+00, -2.3570e-02, -4.8931e-01,\n",
       "         -1.2256e+00,  2.0615e+00,  1.2602e+00, -9.6038e-01, -3.2274e-02,\n",
       "         -6.4393e-01,  5.9620e-01,  1.7711e-01,  1.1517e+00,  7.6437e-01,\n",
       "          4.8272e-01,  1.0268e+00,  7.4599e-01, -1.0638e-01,  8.0193e-01,\n",
       "         -8.3958e-01,  7.3929e-01,  5.1022e-01,  9.3816e-01, -2.2969e-01,\n",
       "          6.1165e-01,  1.8311e-01, -1.1351e-01,  3.2022e-01, -2.0328e+00,\n",
       "         -2.8329e-02, -1.2480e+00],\n",
       "        [ 3.2033e-01,  1.7559e+00, -6.8414e-02, -1.7477e-02, -3.3555e-01,\n",
       "          4.4865e-02, -1.5194e+00, -3.0103e-01,  2.0919e-01,  1.3198e-01,\n",
       "         -2.9152e-01, -2.8409e-01, -2.9855e-01, -7.8126e-01, -3.0143e-02,\n",
       "          1.8586e-02,  9.0803e-01, -9.0794e-01, -7.3948e-01,  2.1253e-01,\n",
       "         -2.5626e-01,  8.5146e-01,  2.5467e-01, -1.6498e+00, -7.9239e-01,\n",
       "         -9.5502e-01,  1.9736e+00,  6.3318e-01, -1.1528e+00, -1.2280e-01,\n",
       "         -8.5798e-01,  1.4498e+00],\n",
       "        [ 1.3623e+00, -1.3835e+00,  1.4856e-01,  8.3659e-01, -1.0776e+00,\n",
       "          1.3340e+00,  1.0278e+00,  1.2602e+00,  5.5020e-01,  9.6797e-01,\n",
       "          4.0167e-01,  9.0674e-01,  5.6705e-01, -2.1030e+00,  3.2297e-01,\n",
       "         -8.3243e-01,  8.8382e-01,  2.5829e-01,  1.8832e-01,  4.0069e-01,\n",
       "          2.6032e-02, -9.4302e-01,  8.2253e-01, -3.1377e-01, -6.0027e-01,\n",
       "          1.0353e+00,  5.1467e-01,  7.8781e-01,  8.6133e-01, -5.2395e-01,\n",
       "          3.5137e-01,  1.2420e+00],\n",
       "        [-8.2550e-01,  9.7325e-01,  1.5343e+00, -9.8356e-01,  7.1707e-01,\n",
       "          1.3163e+00,  4.0050e-03, -4.3520e-01, -8.9369e-02, -1.3786e+00,\n",
       "         -4.7215e-01,  1.4242e+00,  1.2491e+00, -1.1766e+00,  2.2472e-01,\n",
       "         -8.6611e-01, -6.6548e-01,  9.7667e-01, -4.3595e-01, -6.1216e-01,\n",
       "         -5.4193e-01,  2.0573e-01,  4.2303e-01,  1.8309e+00, -1.6185e+00,\n",
       "          1.7875e+00,  1.0507e+00, -5.3342e-01,  3.3550e-01,  5.7734e-01,\n",
       "          8.8263e-01,  1.8843e-01],\n",
       "        [ 2.9084e-01, -3.7089e-01, -7.1707e-01, -8.7924e-01, -1.1715e+00,\n",
       "         -1.9280e-01,  7.7879e-01,  5.5888e-01,  4.6388e-01,  1.1190e+00,\n",
       "         -1.5308e-01, -1.1336e+00,  2.4985e+00, -2.4827e-01,  1.1050e+00,\n",
       "          1.1470e+00,  4.3461e-01,  1.5356e+00, -2.9425e-01, -1.0254e+00,\n",
       "          9.9130e-01,  5.7837e-01, -1.9807e-01, -2.6669e-01,  6.5215e-02,\n",
       "         -3.5030e-02,  2.3372e+00,  7.3866e-02,  2.0212e+00,  1.7746e+00,\n",
       "         -6.4361e-01, -6.4332e-01],\n",
       "        [-1.7531e-01, -1.8443e+00, -1.1290e-01, -5.0350e-01, -1.5683e-01,\n",
       "          1.1845e+00,  2.2246e-01,  1.4069e+00, -1.0857e+00,  2.1711e-01,\n",
       "          6.9341e-01,  8.8400e-01, -4.0634e-01, -2.5521e-01, -4.6771e-01,\n",
       "          1.0286e+00,  1.1801e+00,  2.4930e-02, -1.7834e-01, -5.4211e-01,\n",
       "          5.2144e-01, -8.2629e-01, -1.3869e+00,  2.1766e+00, -1.1066e+00,\n",
       "          8.1593e-01, -1.9865e-01, -1.8679e-01,  1.2231e+00,  2.8628e-01,\n",
       "         -8.6059e-01,  2.5153e-01],\n",
       "        [-1.1465e+00, -5.1922e-04,  1.2932e+00, -9.5250e-01,  2.3625e+00,\n",
       "         -8.5314e-01,  1.0385e+00,  7.4066e-01,  1.7052e+00,  1.0377e+00,\n",
       "         -7.9565e-01,  4.2163e-02,  5.5484e-01,  4.9353e-01,  2.2175e-01,\n",
       "         -1.1688e+00, -7.3207e-01, -5.8158e-01, -1.0040e+00,  6.2362e-02,\n",
       "          6.5693e-01, -3.9466e-01, -2.9388e-01,  1.5185e+00, -1.2885e+00,\n",
       "          5.2533e-01,  1.2038e-01,  4.6349e-02,  1.0489e+00, -1.2649e+00,\n",
       "         -1.3323e-02,  6.5063e-02],\n",
       "        [-3.0752e-01, -8.6731e-01,  2.9181e-01,  7.0734e-01, -3.5939e-01,\n",
       "          8.3878e-01,  1.6053e+00, -1.7214e-01,  3.1456e-02, -5.3107e-01,\n",
       "         -3.5784e-01,  1.4080e+00, -1.4195e+00,  2.2873e-01,  3.6605e-01,\n",
       "          1.1331e+00, -1.9163e+00,  2.3383e-01, -1.2076e+00,  8.0097e-01,\n",
       "          1.8744e+00,  3.8881e-01,  1.5147e+00, -3.8421e-01, -2.7249e-01,\n",
       "          1.1747e+00, -1.7216e+00,  9.3369e-01,  1.0039e+00, -1.5043e+00,\n",
       "          4.9744e-01, -3.0461e-01]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "embedded_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34ecfcae-dea5-4df4-963e-6526ae7da84f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(20000, 32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b2c9aea-e289-4f56-8fd7-e827c559a13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vocabulary size - 20000 Embedding Size - 32\n",
    "#Every time you run this, the tokens are changing -->Everytime you do this, the weights get optimized. At first iteration, it randomly initializes the vectors\n",
    "#During training, the vectors change weights. Everytime model sees new sentense or similar sentence, the weight will change.\n",
    "#You can also do pre trained embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bf384e-7246-4703-af42-ab0d5be36477",
   "metadata": {},
   "source": [
    " A higher embedding dimension allows the model to capture more complex and nuanced features of the data, but increases memory and computation requirements.\n",
    "\n",
    "- A lower embedding dimension is more efficient, but might not capture all the important information, especially for complex tasks or large vocabularies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46f0c62-d8a0-4378-82b4-692c11826c60",
   "metadata": {},
   "source": [
    "These embeddings will need to be learnt when training any model that uses an embedding layer. We can easily compute the number of parameters that need to be learnt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b3975876-c635-45d5-b8e6-1fc9f2fe0cb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of embedding parameters = 640,000\n"
     ]
    }
   ],
   "source": [
    "# How many total number of embeddings in our model? 20000 (Vocab Size)*Embedding Dimension (32)\n",
    "\n",
    "n_embedding_params = sum(len(p.flatten()) for p in embedding_layer.parameters())\n",
    "print(f\"number of embedding parameters = {n_embedding_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d5e92d-f8cb-4ba7-99d7-083ce1245aeb",
   "metadata": {},
   "source": [
    "## Basic Self-Attention\n",
    "\n",
    "An approach to computing attention is to express the new context-aware embeddings as a weighted linear combination or the input embeddings - e.g., $\\vec{x_{i}} \\to \\vec{z_{i}} = \\sum_{j=1}^{N}{a_{ij} \\times \\vec{x_{j}}}$. \n",
    "\n",
    "One sensible approach to computing the weights is to use the vector [dot product](https://en.wikipedia.org/wiki/Dot_product) between the embedding vectors - e.g., $a_{ij} = x_{i}^{T} \\cdot x_{i}$. This will lead to weights that are higher for embedding vectors that are geometrically nearer to one another in the embedding space (i.e., are semantically closer), and vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "880ed329-1400-41f0-a068-9d50c618b357",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.3578e+26, 1.9912e-42, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute the weight by using dot product between embedding vectors - random values: allocates uninitialized memory to tensor\n",
    "torch.empty(n_tokens,n_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd21c5e-46c0-45ec-abfa-c1bf223e1b8a",
   "metadata": {},
   "source": [
    "Self-attention in transformers computes **how much one token should consider every other token’s information to build context-aware representations.** \n",
    "\n",
    "Below code computes the core **similarity scores (unnormalized attention weights) using dot products between embeddings.**\n",
    "\n",
    "For each token \n",
    "\n",
    "i = (acting as a query), we calculate the dot product with every other token j's\n",
    "\n",
    "j = embedding vector (acting as keys)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c7f2cf93-4af7-42aa-aea5-e668e792f339",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9, 9])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#for all tokens --> attention weight is dot product of each embedding token against any other embedding token\n",
    "\n",
    "attn_weights = torch.empty(n_tokens, n_tokens)\n",
    "\n",
    "\n",
    "for i in range(n_tokens):\n",
    "    for j in range(n_tokens):\n",
    "        attn_weights[i, j] = torch.dot(embedded_tokens[i], embedded_tokens[j])\n",
    "\n",
    "attn_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5e897cb2-6dee-487f-9223-9dd8042a38b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[26.1343,  8.3032, -1.3191, -4.6427,  6.6822,  5.4403, -1.2678,  1.9402,\n",
       "         -0.4022],\n",
       "        [ 8.3032, 29.6948, -7.4185, -0.3402, -3.0784,  3.4283,  4.1866,  3.2277,\n",
       "          6.2666],\n",
       "        [-1.3191, -7.4185, 22.4683, -0.2169, -2.1722, -0.0726, -8.7021, -5.5650,\n",
       "         -9.5592],\n",
       "        [-4.6427, -0.3402, -0.2169, 25.5506,  3.4426,  4.8543,  7.5582, -0.0986,\n",
       "          5.0680],\n",
       "        [ 6.6822, -3.0784, -2.1722,  3.4426, 29.5835,  1.6087,  4.6315,  9.4329,\n",
       "          0.6769],\n",
       "        [ 5.4403,  3.4283, -0.0726,  4.8543,  1.6087, 34.1028,  4.1000, -0.4884,\n",
       "         -7.0386],\n",
       "        [-1.2678,  4.1866, -8.7021,  7.5582,  4.6315,  4.1000, 24.8313,  3.0486,\n",
       "          1.7539],\n",
       "        [ 1.9402,  3.2277, -5.5650, -0.0986,  9.4329, -0.4884,  3.0486, 27.9390,\n",
       "          4.2726],\n",
       "        [-0.4022,  6.2666, -9.5592,  5.0680,  0.6769, -7.0386,  1.7539,  4.2726,\n",
       "         31.5849]], grad_fn=<CopySlices>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "50fab80c-ac45-4cdc-9aeb-903ece338280",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[26.1343,  8.3032, -1.3191, -4.6427,  6.6822,  5.4403, -1.2678,  1.9402,\n",
       "         -0.4022],\n",
       "        [ 8.3032, 29.6948, -7.4185, -0.3402, -3.0784,  3.4283,  4.1866,  3.2277,\n",
       "          6.2666],\n",
       "        [-1.3191, -7.4185, 22.4683, -0.2169, -2.1722, -0.0726, -8.7021, -5.5650,\n",
       "         -9.5592],\n",
       "        [-4.6427, -0.3402, -0.2169, 25.5506,  3.4426,  4.8543,  7.5582, -0.0986,\n",
       "          5.0680],\n",
       "        [ 6.6822, -3.0784, -2.1722,  3.4426, 29.5835,  1.6087,  4.6315,  9.4329,\n",
       "          0.6769],\n",
       "        [ 5.4403,  3.4283, -0.0726,  4.8543,  1.6087, 34.1028,  4.1000, -0.4884,\n",
       "         -7.0386],\n",
       "        [-1.2678,  4.1866, -8.7021,  7.5582,  4.6315,  4.1000, 24.8313,  3.0486,\n",
       "          1.7539],\n",
       "        [ 1.9402,  3.2277, -5.5650, -0.0986,  9.4329, -0.4884,  3.0486, 27.9390,\n",
       "          4.2726],\n",
       "        [-0.4022,  6.2666, -9.5592,  5.0680,  0.6769, -7.0386,  1.7539,  4.2726,\n",
       "         31.5849]], grad_fn=<CopySlices>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403a8207-cfaa-4b6d-9269-ffdc10a4a527",
   "metadata": {},
   "source": [
    "This calculation can also be computed more efficiently using matrix multiplication.\n",
    "\n",
    "\n",
    "#### OR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1f56b535-0687-4d8c-b87e-5a3a4e121630",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_weights_matmul = torch.matmul(embedded_tokens, embedded_tokens.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "03b0749d-b0dc-4d2a-8c86-affff4e18ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dot scale product between embedded tokens and their transpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "341f188e-86ca-46ce-9978-7756e8ffff77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[26.1343,  8.3032, -1.3191, -4.6427,  6.6822,  5.4403, -1.2678,  1.9402,\n",
       "         -0.4022],\n",
       "        [ 8.3032, 29.6948, -7.4185, -0.3402, -3.0784,  3.4283,  4.1866,  3.2277,\n",
       "          6.2666],\n",
       "        [-1.3191, -7.4185, 22.4683, -0.2169, -2.1722, -0.0726, -8.7021, -5.5650,\n",
       "         -9.5592],\n",
       "        [-4.6427, -0.3402, -0.2169, 25.5506,  3.4426,  4.8543,  7.5582, -0.0986,\n",
       "          5.0680],\n",
       "        [ 6.6822, -3.0784, -2.1722,  3.4426, 29.5835,  1.6087,  4.6315,  9.4329,\n",
       "          0.6769],\n",
       "        [ 5.4403,  3.4283, -0.0726,  4.8543,  1.6087, 34.1028,  4.1000, -0.4884,\n",
       "         -7.0386],\n",
       "        [-1.2678,  4.1866, -8.7021,  7.5582,  4.6315,  4.1000, 24.8313,  3.0486,\n",
       "          1.7539],\n",
       "        [ 1.9402,  3.2277, -5.5650, -0.0986,  9.4329, -0.4884,  3.0486, 27.9390,\n",
       "          4.2726],\n",
       "        [-0.4022,  6.2666, -9.5592,  5.0680,  0.6769, -7.0386,  1.7539,  4.2726,\n",
       "         31.5849]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weights_matmul"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a503aa9b-1de4-4062-8232-3a8f437a2f5f",
   "metadata": {},
   "source": [
    "And we can verify that the two approaches are equivalent.\n",
    "\n",
    "#### VALIDATING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c5b250bf-03fd-457f-aea8-90678c5b71d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check if both approaches are same or not\n",
    "torch.allclose(attn_weights_matmul, attn_weights, atol=1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e277c8b-f305-4521-b41a-57a3de489678",
   "metadata": {},
   "source": [
    "# Let's break down the key points:\n",
    "\n",
    "#### \"the weights are scaled by the embedding dimension\":\n",
    "\n",
    "1. In the context of self-attention (a core component of transformers), the attention mechanism calculates \"attention scores\" (or \"logits\") between a query and all keys. These scores determine how much focus each part of the input sequence should get.\n",
    "\n",
    "\n",
    "2. Before applying the softmax function, these attention scores are typically divided by the square root of the embedding dimension \n",
    "\n",
    "\n",
    "3. Why? This scaling is crucial to prevent the dot products (which are used to calculate the attention scores) from becoming very large as the embedding dimension increases. When dot products become very large, they can push the softmax function into regions where its gradients are extremely small (saturate), which can lead to vanishing gradients and hinder the training process. Dividing by square root of d, helps to keep the variance of the dot products consistent, regardless of the embedding dimension size. This concept is detailed in the original \"Attention Is All You Need\" paper by Vaswani et al.\n",
    "\n",
    "\n",
    "\n",
    "#### \"and subsequently renormalised to sum to one across rows using the softmax function.\":\n",
    "\n",
    "1. After scaling, the attention scores are passed through a softmax function.\n",
    "\n",
    "\n",
    "2. Why? The softmax function converts the raw attention scores into a probability distribution. This means that for each query, the attention weights assigned to all keys will be positive and sum up to 1. These normalized weights then determine how much each key contributes to the weighted sum that forms the attention output. This effectively creates a convex combination of the value vectors, where the weights indicate the \"importance\" of each value.\n",
    "\n",
    "\n",
    "#### \"Steps like these make models easier to train by normalising the magnitude of gradients used within algorithms like stochastic gradient descent.\":\n",
    "\n",
    "1. Normalization of Gradients: The scaling by square root of d and the use of softmax both contribute to a more stable training process.\n",
    "\n",
    "2. Scaling: As mentioned, it prevents large attention scores from leading to saturated softmax outputs, which would result in near-zero gradients during backpropagation. By keeping the gradients in a reasonable range, the optimization algorithm (like Stochastic Gradient Descent - SGD) can make meaningful updates to the model's weights.\n",
    "\n",
    "3. Softmax: By producing probabilities that sum to one, softmax ensures that the output of the attention mechanism is well-bounded. This helps in controlling the magnitude of activations and, consequently, the gradients flowing back through the network.\n",
    "\n",
    "4. Easier Training: When gradients are well-behaved (not too large, not too small), optimizers can more effectively navigate the loss landscape, leading to faster convergence and better overall model performance. Issues like vanishing or exploding gradients can significantly impede or even halt the training process.\n",
    "\n",
    "\n",
    "In summary, the described steps are fundamental to the stability and effectiveness of attention mechanisms in transformers. They address potential numerical issues that arise from large dot products, ensuring that the gradients during training remain in a healthy range, thus facilitating the training of these complex models with algorithms like SGD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "52fe344c-1268-449d-b1ce-6de53559e2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention gives different weights to different tokens. Self attention gives focus to itself + other tokens\n",
    "# Attention mechanism works on attention score:\n",
    "# Attention Score = Q*(K^T)/sqrt(dk)\n",
    "#Given in attention is all you need by Vaswani and other researchers\n",
    "#Thus, attention score is calculated by using Query and product of transformatoin of K. This was statistically very high. \n",
    "#Model started having vanishing gradiant. To lower this, we started dividing it by square root of dimension of key matrix\n",
    "# Then you normalize things using softmax functions\n",
    "# Earlier we used raw similarity scores, but for a good model, we need attention scores - Let's calculate that. Here, we took raw weights and normalized\n",
    "# it using softmax function and embedding dimension. All values between 0 and 1 because we have used softmax normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7a38788e-87e6-4b9a-84b4-285f25c01ec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[8.7434e-01, 3.7388e-02, 6.8233e-03, 3.7917e-03, 2.8072e-02, 2.2539e-02,\n",
      "         6.8856e-03, 1.2140e-02, 8.0240e-03],\n",
      "        [2.1139e-02, 9.2763e-01, 1.3124e-03, 4.5867e-03, 2.8267e-03, 8.9293e-03,\n",
      "         1.0210e-02, 8.6182e-03, 1.4748e-02],\n",
      "        [1.3762e-02, 4.6819e-03, 9.2241e-01, 1.6723e-02, 1.1836e-02, 1.7155e-02,\n",
      "         3.7314e-03, 6.4970e-03, 3.2068e-03],\n",
      "        [4.1790e-03, 8.9412e-03, 9.1381e-03, 8.6918e-01, 1.7450e-02, 2.2397e-02,\n",
      "         3.6122e-02, 9.3312e-03, 2.3259e-02],\n",
      "        [1.6043e-02, 2.8571e-03, 3.3535e-03, 9.0482e-03, 9.1935e-01, 6.5429e-03,\n",
      "         1.1164e-02, 2.6089e-02, 5.5492e-03],\n",
      "        [6.1195e-03, 4.2879e-03, 2.3093e-03, 5.5173e-03, 3.1085e-03, 9.7101e-01,\n",
      "         4.8285e-03, 2.1456e-03, 6.7401e-04],\n",
      "        [8.4190e-03, 2.2081e-02, 2.2621e-03, 4.0073e-02, 2.3887e-02, 2.1745e-02,\n",
      "         8.4911e-01, 1.8057e-02, 1.4363e-02],\n",
      "        [9.1374e-03, 1.1473e-02, 2.4245e-03, 6.3722e-03, 3.4360e-02, 5.9479e-03,\n",
      "         1.1115e-02, 9.0537e-01, 1.3800e-02],\n",
      "        [3.3564e-03, 1.0911e-02, 6.6506e-04, 8.8273e-03, 4.0617e-03, 1.0384e-03,\n",
      "         4.9136e-03, 7.6695e-03, 9.5856e-01]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "attn_weights_norm = F.softmax(attn_weights / math.sqrt(EMBEDDING_DIM), dim=1)\n",
    "print(attn_weights_norm)\n",
    "#attn_weights: This variable represents the raw, unnormalized attention scores (or \"logits\").\n",
    "#Softmax converts the scaled attention scores into a probability distribution. The output values will be between 0 and 1,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd875e5c-c909-4ec2-9ea6-2248dfd602d8",
   "metadata": {},
   "source": [
    "Verify that rows sum to one. The softmax function is not changing because the moment you apply dot product, it will remain constant throughout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2437a0d0-bf17-4d6c-b31e-ad11e85efe69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "       grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weights_norm.sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fddc7f22-319f-44c3-8d10-4bf5c7a8eedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Total values sum to 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6198e0db-95bc-4aa5-9a8f-1831d4901d0c",
   "metadata": {},
   "source": [
    "### Now we need context aware embeddings: What are context aware embeddings?\n",
    "### Embedding is able to read context behind each token\n",
    "Context-aware embeddings (like those from BERT, GPT, RoBERTa) generate vectors for words or sentences depending on their surrounding context. So, “bank” in “river bank” and “bank” in “credit bank” have different embeddings.\n",
    "\n",
    "Value vector helps us calculate the context aware embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2075708b-d4ff-4e88-b128-92629905faa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9, 32])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_weighted_embeddings = torch.matmul(attn_weights_norm, embedded_tokens)\n",
    "context_weighted_embeddings.shape\n",
    "\n",
    "#creating a weighted sum of the input embeddings (which act as \"values\") based on the calculated attention weights. \n",
    "#This weighted sum forms the \"context vector\" or the output of the attention head for each query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8e70feff-12b6-4110-ad31-f4d7ffebb754",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.0204e+00,  8.7015e-01, -2.4918e-01,  1.2211e+00,  1.0704e+00,\n",
       "         -5.2672e-01,  7.7677e-03,  5.0431e-01, -3.7671e-01, -1.7926e+00,\n",
       "         -8.8512e-01,  5.6510e-01,  5.3660e-01,  6.7693e-01,  3.1917e-01,\n",
       "          1.2565e-01,  4.3635e-01,  4.1690e-01,  5.4701e-01, -8.3406e-02,\n",
       "         -1.3436e-01,  6.1509e-01,  1.1197e+00,  5.8140e-01,  7.0009e-01,\n",
       "          3.9966e-01,  1.4699e+00, -6.2345e-01,  2.0834e+00,  1.0477e-01,\n",
       "         -6.8040e-01, -3.4457e-01],\n",
       "        [-1.1314e+00, -3.1196e-01, -2.2292e+00, -7.5803e-04, -4.2823e-01,\n",
       "         -1.1242e+00,  1.9543e+00,  1.2069e+00, -8.8895e-01, -5.9235e-02,\n",
       "         -6.2384e-01,  5.9320e-01,  1.8282e-01,  1.0736e+00,  7.2972e-01,\n",
       "          4.7152e-01,  9.4720e-01,  7.1531e-01, -1.1649e-01,  7.4002e-01,\n",
       "         -7.3636e-01,  6.9609e-01,  5.0881e-01,  9.1078e-01, -2.2827e-01,\n",
       "          6.1326e-01,  2.0561e-01, -1.0442e-01,  4.0297e-01, -1.8972e+00,\n",
       "         -4.7123e-02, -1.1640e+00],\n",
       "        [ 2.8390e-01,  1.6043e+00, -6.0337e-02, -1.4685e-02, -3.1087e-01,\n",
       "          6.3554e-02, -1.3502e+00, -2.2988e-01,  2.0593e-01,  1.1829e-01,\n",
       "         -2.9031e-01, -2.3059e-01, -2.0287e-01, -7.5492e-01,  7.6898e-03,\n",
       "          1.6474e-02,  8.5675e-01, -7.8941e-01, -6.9132e-01,  1.8136e-01,\n",
       "         -2.1918e-01,  7.9007e-01,  2.6758e-01, -1.4812e+00, -7.6107e-01,\n",
       "         -8.2511e-01,  1.8987e+00,  5.8449e-01, -9.6287e-01, -1.0384e-01,\n",
       "         -7.9871e-01,  1.3391e+00],\n",
       "        [ 1.1398e+00, -1.2633e+00,  1.3146e-01,  6.8539e-01, -9.4460e-01,\n",
       "          1.2194e+00,  9.6993e-01,  1.1646e+00,  4.5606e-01,  8.3987e-01,\n",
       "          3.3434e-01,  8.5783e-01,  5.2910e-01, -1.8469e+00,  3.1086e-01,\n",
       "         -6.5535e-01,  7.7700e-01,  2.7684e-01,  1.0073e-01,  3.2299e-01,\n",
       "          9.3440e-02, -8.1047e-01,  7.1236e-01, -1.6727e-01, -6.1240e-01,\n",
       "          9.9017e-01,  4.9819e-01,  6.9430e-01,  8.7895e-01, -4.6060e-01,\n",
       "          2.7539e-01,  1.0717e+00],\n",
       "        [-7.9800e-01,  8.7545e-01,  1.4305e+00, -9.0569e-01,  7.1694e-01,\n",
       "          1.2036e+00,  5.5383e-02, -3.4037e-01, -5.0362e-02, -1.2573e+00,\n",
       "         -4.6460e-01,  1.3390e+00,  1.1796e+00, -1.0779e+00,  2.2612e-01,\n",
       "         -8.0560e-01, -6.0396e-01,  9.0197e-01, -4.2772e-01, -5.6431e-01,\n",
       "         -4.6388e-01,  1.8252e-01,  4.0261e-01,  1.7464e+00, -1.5294e+00,\n",
       "          1.6860e+00,  1.0097e+00, -4.8806e-01,  4.0965e-01,  4.9609e-01,\n",
       "          7.8773e-01,  1.7881e-01],\n",
       "        [ 2.7286e-01, -3.6542e-01, -7.0019e-01, -8.4740e-01, -1.1326e+00,\n",
       "         -1.8012e-01,  7.7086e-01,  5.6431e-01,  4.4543e-01,  1.0781e+00,\n",
       "         -1.5578e-01, -1.0806e+00,  2.4345e+00, -2.4846e-01,  1.0789e+00,\n",
       "          1.1126e+00,  4.3705e-01,  1.4980e+00, -2.8785e-01, -9.9395e-01,\n",
       "          9.6107e-01,  5.6162e-01, -1.8238e-01, -2.3789e-01,  4.9375e-02,\n",
       "         -1.4310e-02,  2.2886e+00,  7.0876e-02,  1.9899e+00,  1.7118e+00,\n",
       "         -6.3100e-01, -6.2021e-01],\n",
       "        [-1.6736e-01, -1.6134e+00, -9.6676e-02, -4.3193e-01, -1.4839e-01,\n",
       "          1.0511e+00,  3.2996e-01,  1.2891e+00, -8.8491e-01,  2.0798e-01,\n",
       "          5.4784e-01,  8.3473e-01, -2.4098e-01, -2.9208e-01, -3.2626e-01,\n",
       "          8.5120e-01,  1.0190e+00,  9.8762e-02, -1.9444e-01, -4.5112e-01,\n",
       "          4.7078e-01, -6.9968e-01, -1.1001e+00,  1.9171e+00, -1.0274e+00,\n",
       "          8.1692e-01, -7.2895e-02, -1.3097e-01,  1.1820e+00,  1.8633e-01,\n",
       "         -7.1181e-01,  2.2346e-01],\n",
       "        [-1.0849e+00, -7.7909e-04,  1.1930e+00, -8.7888e-01,  2.1478e+00,\n",
       "         -7.1451e-01,  9.9497e-01,  6.9853e-01,  1.5210e+00,  8.8124e-01,\n",
       "         -7.4900e-01,  1.2687e-01,  5.4559e-01,  4.1027e-01,  2.2834e-01,\n",
       "         -1.0526e+00, -6.7247e-01, -4.6882e-01, -9.3997e-01,  4.5794e-02,\n",
       "          6.0221e-01, -3.4009e-01, -2.2413e-01,  1.4648e+00, -1.2380e+00,\n",
       "          5.7663e-01,  1.5760e-01,  3.3657e-02,  1.0278e+00, -1.1578e+00,\n",
       "          4.4320e-03,  5.4087e-02],\n",
       "        [-3.1182e-01, -8.4804e-01,  2.6874e-01,  6.7535e-01, -3.3639e-01,\n",
       "          8.0487e-01,  1.5788e+00, -1.2725e-01,  3.1128e-02, -5.0307e-01,\n",
       "         -3.5466e-01,  1.3752e+00, -1.3423e+00,  2.1284e-01,  3.6449e-01,\n",
       "          1.0782e+00, -1.8178e+00,  2.3647e-01, -1.1659e+00,  7.7416e-01,\n",
       "          1.7935e+00,  3.6955e-01,  1.4615e+00, -3.3057e-01, -2.8834e-01,\n",
       "          1.1577e+00, -1.6305e+00,  8.9610e-01,  9.9779e-01, -1.4724e+00,\n",
       "          4.7501e-01, -2.9301e-01]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_weighted_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5dbd2891-8e0b-4473-beba-e9021630f2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedded_tokens: are the value vectors (V).\n",
    "#These are the actual embeddings of the input tokens (or features) from which information is being extracted. \n",
    "#Their shape is usually (batch_size, num_tokens, embedding_dim) (where num_tokens corresponds to num_keys).\n",
    "#Each row represents the embedding vector of a particular token in the sequence.\n",
    "#More the embedding size, it extracts more data features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "22ac420a-c16a-443e-a2dc-0182127b0fed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.1398, -1.2633,  0.1315,  0.6854, -0.9446,  1.2194,  0.9699,  1.1646,\n",
       "         0.4561,  0.8399,  0.3343,  0.8578,  0.5291, -1.8469,  0.3109, -0.6554,\n",
       "         0.7770,  0.2768,  0.1007,  0.3230,  0.0934, -0.8105,  0.7124, -0.1673,\n",
       "        -0.6124,  0.9902,  0.4982,  0.6943,  0.8789, -0.4606,  0.2754,  1.0717],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_weighted_embeddings_3 = (\n",
    "    attn_weights_norm[3, 0] * embedded_tokens[0]\n",
    "    + attn_weights_norm[3, 1] * embedded_tokens[1]\n",
    "    + attn_weights_norm[3, 2] * embedded_tokens[2]\n",
    "    + attn_weights_norm[3, 3] * embedded_tokens[3]\n",
    "    + attn_weights_norm[3, 4] * embedded_tokens[4]\n",
    "    + attn_weights_norm[3, 5] * embedded_tokens[5]\n",
    "    + attn_weights_norm[3, 6] * embedded_tokens[6]\n",
    "    + attn_weights_norm[3, 7] * embedded_tokens[7]\n",
    "    + attn_weights_norm[3, 8] * embedded_tokens[8]\n",
    "    )\n",
    "\n",
    "context_weighted_embeddings_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "28058555-debe-4de5-8506-b71737150c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Third tokens weight printed here , similarly you can check for other"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c32c40-ffa2-4390-a9ed-6ad61e4b38d1",
   "metadata": {},
   "source": [
    "And verifying the output against the matrix multiplication computed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "73b2750d-8650-416f-a117-776d42e352af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(context_weighted_embeddings_3, context_weighted_embeddings[3], atol=1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f5c7ee-5eb7-4cf2-88a4-ad622d7eda5b",
   "metadata": {},
   "source": [
    "# Another Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e941441-76dc-4812-b7c9-fe8157e60056",
   "metadata": {},
   "source": [
    "#### 1. Setup and Imports\n",
    "\n",
    "First, let's import the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d6d3f81f-51e2-4691-a960-0d452db73948",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "# For better display in notebooks\n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb5d703-783c-4ba2-96f1-92846cc9778c",
   "metadata": {},
   "source": [
    "#### 2. Define Hyperparameters and Input Data\n",
    "\n",
    "We'll use small, illustrative dimensions and a simple \"sequence\" of embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6822d3c3-1641-4a00-9fbe-17f8c09d105f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**`embedded_tokens` (simulated input/value vectors) Shape:** `torch.Size([5, 4])`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.1000, 0.2000, 0.3000, 0.4000],\n",
       "        [0.5000, 0.6000, 0.7000, 0.8000],\n",
       "        [0.9000, 0.0000, 0.1000, 0.2000],\n",
       "        [0.3000, 0.4000, 0.5000, 0.6000],\n",
       "        [0.7000, 0.8000, 0.9000, 0.0000]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define embedding dimension (dk)\n",
    "EMBEDDING_DIM = 4\n",
    "\n",
    "# Define sequence length (number of tokens/words)\n",
    "SEQUENCE_LENGTH = 5\n",
    "\n",
    "# --- Simulate Input Embeddings (Our \"Value\" vectors initially) ---\n",
    "# In a real model, these would come from an embedding layer\n",
    "# or previous transformer block.\n",
    "# Let's represent 5 tokens, each with an embedding of 4 dimensions.\n",
    "# Shape: (sequence_length, embedding_dim)\n",
    "embedded_tokens = torch.tensor([\n",
    "    [0.1, 0.2, 0.3, 0.4], # Token 0: \"The\"\n",
    "    [0.5, 0.6, 0.7, 0.8], # Token 1: \"cat\"\n",
    "    [0.9, 0.0, 0.1, 0.2], # Token 2: \"sat\"\n",
    "    [0.3, 0.4, 0.5, 0.6], # Token 3: \"on\"\n",
    "    [0.7, 0.8, 0.9, 0.0]  # Token 4: \"mat\"\n",
    "], dtype=torch.float32)\n",
    "\n",
    "display(Markdown(f\"**`embedded_tokens` (simulated input/value vectors) Shape:** `{embedded_tokens.shape}`\"))\n",
    "display(embedded_tokens)\n",
    "print(\"\\n---\\n\")\n",
    "\n",
    "# --- Simulate Query and Key Matrices ---\n",
    "# In self-attention, Q, K, V typically come from linear transformations\n",
    "# of the same input embeddings. For simplicity, let's just make Q, K same as V for now.\n",
    "# In a real scenario, you'd have:\n",
    "# Q = embedded_tokens @ W_q\n",
    "# K = embedded_tokens @ W_k\n",
    "# V = embedded_tokens @ W_v\n",
    "# where W_q, W_k, W_v are weight matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "36f05b74-0a1e-48a7-855a-5719fc8363e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue in day 2 exercise , read theory in day 1 theory part"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a15577-5b9b-4f0d-85c2-124d371b2cc3",
   "metadata": {},
   "source": [
    "# CAUSAL MASKING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f71efa-e2ae-45c2-ac5a-e931f909b626",
   "metadata": {},
   "source": [
    "In a standard Transformer encoder's self-attention, each word can attend to all other words in the input sequence, both before and after it. This is great for understanding context in a static sentence.\n",
    "\n",
    "\n",
    "However, imagine you're training a model to write a story, one word at a time. If, when predicting the third word, the model could \"see\" the actual fourth, fifth, or sixth words (which haven't been generated yet), it would be \"cheating.\" It wouldn't be learning to predict genuinely based only on what has come before. This is a problem known as **data leakage or look-ahead bias.**\n",
    "\n",
    "\n",
    "This isn't a problem if all we're doing is creating embeddings (or sequences) based on whole passages of text. It does pose a problem, however, if we're trying to develop a model that can generate new sequences given an initial sequence (or prompt).\n",
    "\n",
    "- For tasks like text classification or summarization where you have the entire input available upfront, looking at future words is fine and even beneficial.\n",
    "\n",
    "- But for tasks like predicting the next word in a sequence or machine translation decoding (generating the target sentence word by word), the model must only rely on information up to the current point. It must operate in an **autoregressive manner.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786af828-67a5-4060-ae15-dead5d56711e",
   "metadata": {},
   "source": [
    "### The Solution: Causal Masking\n",
    "\n",
    "\"This problem is solved by using causal masking.\"\n",
    "Causal masking (also known as **look-ahead masking or masked self-attention)** is a technique applied specifically in the self-attention mechanism of the Transformer's decoder (and decoder-only models like GPT).\n",
    "\n",
    "###### Causal masking matrices can be constructed to flag which attention weights should be set to zero so that causal relationships between embeddings aren't broken.\n",
    "\n",
    "\n",
    "- During the calculation of attention scores, a causal mask is applied. This mask is typically an upper triangular matrix (including the diagonal) filled with very large negative numbers (like negative infinity) in the positions corresponding to \"future\" tokens.\n",
    "\n",
    "- When softmax is applied to these masked scores, the terms become effectively zero. This forces the attention weights for future tokens to be zero, meaning the current token cannot attend to any subsequent tokens in the sequence.\n",
    "\n",
    "**For example, when calculating the representation for the first word, it can only attend to itself. When calculating for the second word, it can attend to the first word and itself, but not the third, fourth, or fifth words, and so on.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b8c8b4a8-03f3-4672-a057-6348a9f84e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mask created in below format and then we will apply to the attention score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a224fb6-9631-49de-94ae-29a2a593d909",
   "metadata": {},
   "source": [
    "# CAUSAL MASKING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "17ca116f-e833-4ef3-b012-c51e82533ad3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False,  True,  True,  True,  True,  True,  True,  True,  True],\n",
       "        [False, False,  True,  True,  True,  True,  True,  True,  True],\n",
       "        [False, False, False,  True,  True,  True,  True,  True,  True],\n",
       "        [False, False, False, False,  True,  True,  True,  True,  True],\n",
       "        [False, False, False, False, False,  True,  True,  True,  True],\n",
       "        [False, False, False, False, False, False,  True,  True,  True],\n",
       "        [False, False, False, False, False, False, False,  True,  True],\n",
       "        [False, False, False, False, False, False, False, False,  True],\n",
       "        [False, False, False, False, False, False, False, False, False]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This line of code specifically constructs the mask that implements the \"causal\" or \"look-ahead\" behavior\n",
    "causal_mask = torch.triu(torch.full((n_tokens, n_tokens), True), diagonal=1)\n",
    "causal_mask\n",
    "\n",
    "#True means \"apply the mask/block this information,\" \n",
    "#False means \"do not mask/allow this information through.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba922ced-41d1-4f23-8e49-a2187bb2cc1a",
   "metadata": {},
   "source": [
    "Apply Mask to the attention score: This causal_mask is precisely the mechanism to prevent the \"cheating\" we talked about in sequence generation.\n",
    "\n",
    "In the matrix above, True at (row, col) means that the token at row (as a query) is forbidden from attending to the token at col (as a key).\n",
    "\n",
    "Since True appears only for respective col and row (i.e., tokens that come after the current query token), this mask ensures that:\n",
    "\n",
    "When the attention mechanism calculates the context for token 0, it can only use information from token 0 itself (not 1, 2, 3).\n",
    "When calculating for token 1, it can use information from token 0 and 1, but not 2 or 3.\n",
    "And so on.\n",
    "\n",
    "During the attention calculation, this boolean mask is typically used to set the attention scores for the \"forbidden\" (future) connections to a very large negative number. After softmax, these large negative numbers effectively become zero, meaning the model assigns no attention to future tokens. This enforces the autoregressive property, allowing the model to learn to predict sequences step-by-step based only on past information.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9332a4ef-9cd3-4871-9dd5-94d93d66148f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.3191e+00, -7.4185e+00,  2.2468e+01, -1.0000e+10, -1.0000e+10,\n",
       "        -1.0000e+10, -1.0000e+10, -1.0000e+10, -1.0000e+10],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This step directly implements the \"causal\" or \"look-ahead\" constraint. \n",
    "# For any token being processed, it prevents its attention mechanism from looking at or gaining information from \n",
    "#tokens that appear later in the sequence.\n",
    "\n",
    "causal_attn_weights = attn_weights.masked_fill(causal_mask, -1e10)\n",
    "causal_attn_weights[2]\n",
    "\n",
    "\n",
    "#-1e10 is a very large negative number (e.g., -10,000,000,000). \n",
    "#It's used so that when this value goes through the softmax function, \n",
    "#it results in an output extremely close to zero, effectively \"masking out\" or nullifying the attention to that position. \n",
    "#This helps out to nullify the vanishing gradiant issue\n",
    "#Learning rate improves the learning of model, causal masking nullifies the attention to future tokens \n",
    "#If you want the model to look everywhere, you make -1e10 very high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "94c12afb-cbbc-4101-865d-165e43f536e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.6134e+01, -1.0000e+10, -1.0000e+10, -1.0000e+10, -1.0000e+10,\n",
       "         -1.0000e+10, -1.0000e+10, -1.0000e+10, -1.0000e+10],\n",
       "        [ 8.3032e+00,  2.9695e+01, -1.0000e+10, -1.0000e+10, -1.0000e+10,\n",
       "         -1.0000e+10, -1.0000e+10, -1.0000e+10, -1.0000e+10],\n",
       "        [-1.3191e+00, -7.4185e+00,  2.2468e+01, -1.0000e+10, -1.0000e+10,\n",
       "         -1.0000e+10, -1.0000e+10, -1.0000e+10, -1.0000e+10],\n",
       "        [-4.6427e+00, -3.4016e-01, -2.1691e-01,  2.5551e+01, -1.0000e+10,\n",
       "         -1.0000e+10, -1.0000e+10, -1.0000e+10, -1.0000e+10],\n",
       "        [ 6.6822e+00, -3.0784e+00, -2.1722e+00,  3.4426e+00,  2.9584e+01,\n",
       "         -1.0000e+10, -1.0000e+10, -1.0000e+10, -1.0000e+10],\n",
       "        [ 5.4403e+00,  3.4283e+00, -7.2561e-02,  4.8543e+00,  1.6087e+00,\n",
       "          3.4103e+01, -1.0000e+10, -1.0000e+10, -1.0000e+10],\n",
       "        [-1.2678e+00,  4.1866e+00, -8.7021e+00,  7.5582e+00,  4.6315e+00,\n",
       "          4.1000e+00,  2.4831e+01, -1.0000e+10, -1.0000e+10],\n",
       "        [ 1.9402e+00,  3.2277e+00, -5.5650e+00, -9.8624e-02,  9.4329e+00,\n",
       "         -4.8842e-01,  3.0486e+00,  2.7939e+01, -1.0000e+10],\n",
       "        [-4.0219e-01,  6.2666e+00, -9.5592e+00,  5.0680e+00,  6.7687e-01,\n",
       "         -7.0386e+00,  1.7539e+00,  4.2726e+00,  3.1585e+01]],\n",
       "       grad_fn=<MaskedFillBackward0>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "causal_attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b429ad0-0f71-4e23-88fa-9340498f2333",
   "metadata": {},
   "source": [
    "You have a big grid of numbers called attn_weights. These numbers tell the model how much to pay attention to each word when trying to understand the sentence.\n",
    "\n",
    "You want to make sure the model only looks at words that came before or at the current word, not words that come after (because the future is unknown). So, you create a \"mask\" that marks the future words.\n",
    "\n",
    "Then you use .masked_fill(causal_mask, -1e10) to replace those future words' attention numbers with a really, really tiny number (like negative one billion). This basically tells the model: \"Ignore those future words.\"\n",
    "\n",
    "The result is causal_attn_weights — the attention numbers but with future words blocked out.\n",
    "\n",
    "When you write causal_attn_weights[2], you are saying: \"Show me the data for the third item in this group.\" What this “third item” is depends on how your data is organized. It could be:\n",
    "\n",
    "The third sentence in a batch of sentences,\n",
    "\n",
    "The third attention head (a smaller attention machine inside the bigger one),\n",
    "\n",
    "Or something else, depending on your setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b71c1b36-2f2f-47e4-be2e-cd0cfcdc19eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "       grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#And apply scaling and normalisation as before.\n",
    "causal_attn_weights_norm = F.softmax(\n",
    "    causal_attn_weights / math.sqrt(EMBEDDING_DIM), dim=1\n",
    ")\n",
    "causal_attn_weights_norm.sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ca39f0cd-c2cc-4dad-a6ed-0dd2bf5ba878",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checked if sums to one"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a509925-3e68-4b87-9939-3eca72af1eb7",
   "metadata": {},
   "source": [
    "From this we prepare the context aware embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7019a329-2162-42d6-98ec-6e0def941076",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9, 32])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "causal_context_weighted_embeddings = torch.matmul(\n",
    "    causal_attn_weights_norm, embedded_tokens\n",
    ")\n",
    "causal_context_weighted_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "201eb126-13a7-445f-b509-9e0b641f8a0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9, 32])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Generate Embeddings again just to align and convert input tokens into vectors --> This is run through neural network.embedding\n",
    "# Vocabulary Size and Dimension of Embeddings --> you tokenize the sentence\n",
    "EMBEDDING_DIM = 32\n",
    "\n",
    "embedding_layer = nn.Embedding(VOCAB_SIZE, EMBEDDING_DIM)\n",
    "embedded_tokens = embedding_layer(tokenized_sentence)\n",
    "embedded_tokens.shape\n",
    "\n",
    "#Torch.size(size of input data, embedding dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "51fe6bce-0b20-4927-a4c7-774d39f5e4bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2.3650,  0.3154,  0.2994, -1.1914,  0.1285, -0.3733, -0.5181,  1.3512,\n",
       "        -1.5681, -1.9776, -0.2771, -0.0908, -0.7488,  0.1672, -0.5036, -1.9478,\n",
       "         0.6731, -0.4910,  1.1733, -0.4775,  1.0050,  1.2333, -2.0507, -0.4148,\n",
       "         1.0885,  0.0528, -0.1399, -0.4349, -0.2876,  0.9835,  0.1909,  0.0569],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check embeddings\n",
    "causal_context_weighted_embeddings[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7ed18aee-f110-4e91-a5de-98677e890a77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Integrity of embeddings\n",
    "causal_context_weighted_embeddings_3 = (\n",
    "    causal_attn_weights_norm[3, 0] * embedded_tokens[0]\n",
    "    + causal_attn_weights_norm[3, 1] * embedded_tokens[1]\n",
    "    + causal_attn_weights_norm[3, 2] * embedded_tokens[2]\n",
    "    + causal_attn_weights_norm[3, 3] * embedded_tokens[3]\n",
    ")\n",
    "\n",
    "torch.allclose(\n",
    "    causal_context_weighted_embeddings_3, causal_context_weighted_embeddings[3]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b7ad73e7-905c-4c99-9d6a-1479589a5113",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.9243, -0.9740, -0.3974,  0.6696,  1.0455, -1.7275,  0.6280,  0.9299,\n",
       "         1.2461, -0.2323,  0.0031, -0.5088, -0.8515,  0.0597, -1.6898, -0.7558,\n",
       "        -0.6331, -0.3503, -1.7318, -1.1988,  0.2001,  2.2014,  0.4385,  1.0417,\n",
       "         1.3765, -0.1261, -0.2348, -1.1892,  0.3214,  0.8378, -1.0172,  0.0480],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "causal_context_weighted_embeddings_3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9182aea5-8209-45ab-8c3f-5d0776159fb8",
   "metadata": {},
   "source": [
    "# Day 3 - Parameterized Self Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b994c1-fe40-4cfd-ad84-92a53f81e115",
   "metadata": {},
   "source": [
    "## Parametrised Self-Attention\n",
    "\n",
    "Parametrised Self-Attention refers to a variation of the self-attention mechanism where additional learnable parameters are introduced into the attention computation. This allows the model to adaptively control how it distributes attention across input tokens.\n",
    "\n",
    "![%7B81956077-0F8C-497A-BDC1-72200A189D94%7D.png](attachment:%7B81956077-0F8C-497A-BDC1-72200A189D94%7D.png)\n",
    "\n",
    "\n",
    "##### What Makes Parametrised Self-Attention Different?\n",
    "\n",
    "While standard self-attention already includes parameters (via the Q, K, V projections), the term **\"parametrised self-attention\" typically refers to further enhancements where:**\n",
    "\n",
    "- Additional parameters are introduced into the attention score calculation or the softmax operation.\n",
    "\n",
    "- Learnable biases, temperature scaling factors, or gating mechanisms are included.\n",
    "\n",
    "- Multi-head attention components may be further parameterised with learned transformations or scaling terms.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff039a7d-7667-41b1-98fa-e53e8062a97a",
   "metadata": {},
   "source": [
    "### Queries, Keys and Values\n",
    "\n",
    "In this setup, the values contain the information that we wish to access via a query that is made on a set of keys (that map to the values), such that the context-aware embeddings can now be computed as,\n",
    "\n",
    "$$\n",
    "\\vec{z_{i}} = \\sum_{j=1}^{N}{a_{ij} \\times \\vec{v_{j}}}\n",
    "$$\n",
    "\n",
    "Where, $a_{ij} = q_{i}^{T} \\cdot k_{i}$ - i.e., the attention weights now represent the distance between the query and keys.\n",
    "\n",
    "Very often we only have a single sequence to work with, so the model will have to learn how to infer the queries, keys and values from this. We can enable this level of plasticity by defining three  $N \\times N$ weight matrices, $\\textbf{U}_{q}$, $\\textbf{U}_{k}$ and $\\textbf{U}_{v}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ef345234-1cc8-4913-8a87-baf0482ba8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Take random Q,K,V tensor values\n",
    "u_q = torch.rand(n_tokens, n_tokens)\n",
    "u_k = torch.rand(n_tokens, n_tokens)\n",
    "u_v = torch.rand(n_tokens, n_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1275c962-7709-4a59-a031-5cfd76b4d575",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6565, 0.0888, 0.7548, 0.5358, 0.1745, 0.1289, 0.2809, 0.7323, 0.6916],\n",
       "        [0.9928, 0.2934, 0.5413, 0.2614, 0.4556, 0.2404, 0.7967, 0.8775, 0.9719],\n",
       "        [0.8782, 0.6002, 0.6184, 0.2610, 0.2850, 0.2901, 0.9675, 0.1707, 0.0308],\n",
       "        [0.4414, 0.0191, 0.2553, 0.3147, 0.4735, 0.7354, 0.1285, 0.3538, 0.0082],\n",
       "        [0.1946, 0.4983, 0.9141, 0.3868, 0.6972, 0.8244, 0.1683, 0.8110, 0.0437],\n",
       "        [0.2777, 0.8364, 0.1628, 0.4824, 0.4077, 0.6239, 0.8953, 0.6745, 0.6595],\n",
       "        [0.2188, 0.3821, 0.1226, 0.2618, 0.7807, 0.2419, 0.0597, 0.2273, 0.3878],\n",
       "        [0.8476, 0.0118, 0.6823, 0.7973, 0.4422, 0.6797, 0.6160, 0.1221, 0.8538],\n",
       "        [0.5712, 0.2830, 0.2578, 0.5347, 0.8326, 0.1532, 0.5764, 0.3492, 0.4376]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u_q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7637cd4e-2bc8-42f7-9238-2eb3ad48947a",
   "metadata": {},
   "source": [
    "From which we can define the query, keys and values as functions of x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5a8e83b8-b52a-49bb-84a0-100529498f9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Metrices\n",
    "q = torch.matmul(u_q, embedded_tokens)\n",
    "k = torch.matmul(u_k, embedded_tokens)\n",
    "v = torch.matmul(u_v, embedded_tokens)\n",
    "\n",
    "q.shape == k.shape == v.shape == embedded_tokens.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36176ce1-8e21-4e63-99d2-4d3aaf0568ca",
   "metadata": {},
   "source": [
    "We then recompute our parameterised attention weights using the same steps we used before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "594084b5-4c99-4366-80bb-0ff3039f3574",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9, 32])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Compare with previoous one and check what different you have done\n",
    "attn_weights_param = torch.empty(n_tokens, n_tokens)\n",
    "\n",
    "for i in range(n_tokens):\n",
    "    for j in range(n_tokens):\n",
    "        attn_weights_param[i, j] = torch.dot(q[i], k[j])\n",
    "\n",
    "attn_weights_param_norm = F.softmax(\n",
    "    attn_weights_param / math.sqrt(EMBEDDING_DIM), dim=1\n",
    ")\n",
    "context_weighted_embeddings_param = torch.matmul(attn_weights_param_norm, v)\n",
    "\n",
    "context_weighted_embeddings_param.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e0deb5f7-1e95-4bd4-a3ee-dd8b7a5288fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.9563e-01, -2.0936e+00, -1.7155e+00, -1.3937e+00, -3.0681e-01,\n",
       "         -1.8992e+00,  2.0013e-01, -1.6290e+00,  9.8174e-01, -1.9686e+00,\n",
       "         -1.4531e+00, -6.4580e-01, -2.5751e+00, -4.6332e-01, -1.8140e+00,\n",
       "         -5.5802e-01, -3.3429e-01, -3.1449e+00, -1.0671e+00,  2.3382e+00,\n",
       "          5.5324e+00,  1.4283e+00,  1.4317e+00,  1.5982e+00,  2.4086e+00,\n",
       "         -3.6630e-01,  1.7945e+00, -5.6358e-01, -1.7490e+00,  2.1675e+00,\n",
       "          1.0573e+00, -1.6814e-02],\n",
       "        [-5.7416e-01, -2.0261e+00, -1.7455e+00, -1.3427e+00, -8.4654e-02,\n",
       "         -1.8744e+00,  8.5526e-02, -1.9644e+00,  1.1028e+00, -1.5476e+00,\n",
       "         -1.4959e+00, -3.4397e-01, -2.7230e+00, -6.2538e-01, -1.9278e+00,\n",
       "         -5.1260e-01, -3.1722e-01, -2.8826e+00, -1.3861e+00,  2.3536e+00,\n",
       "          5.7833e+00,  1.5104e+00,  1.5935e+00,  1.6391e+00,  2.2114e+00,\n",
       "         -4.1484e-01,  1.8567e+00, -2.7050e-01, -1.6071e+00,  1.8552e+00,\n",
       "          9.6431e-01,  2.3026e-02],\n",
       "        [-4.0054e-01, -2.1304e+00, -1.7611e+00, -1.3899e+00, -2.1453e-01,\n",
       "         -1.8622e+00,  1.4983e-01, -1.8975e+00,  8.9477e-01, -1.6242e+00,\n",
       "         -1.4796e+00, -1.7470e-01, -2.8538e+00, -6.4658e-01, -1.7591e+00,\n",
       "         -5.8248e-01, -3.9398e-01, -2.7256e+00, -1.2782e+00,  2.4660e+00,\n",
       "          5.8251e+00,  1.2928e+00,  1.5798e+00,  1.6308e+00,  1.7265e+00,\n",
       "         -4.2421e-01,  1.8147e+00, -1.0769e-01, -1.7071e+00,  1.7502e+00,\n",
       "          9.1739e-01,  2.5718e-03],\n",
       "        [-5.8034e-02, -2.2969e+00, -1.8331e+00, -1.6238e+00, -5.4846e-01,\n",
       "         -1.7127e+00,  1.7776e-01, -1.6862e+00,  3.0899e-01, -2.1217e+00,\n",
       "         -1.5018e+00, -1.2181e-01, -2.9048e+00, -5.4425e-01, -1.3911e+00,\n",
       "         -6.9262e-01, -2.8127e-01, -2.7180e+00, -6.6739e-01,  2.6547e+00,\n",
       "          5.7541e+00,  7.2952e-01,  1.4386e+00,  1.4448e+00,  1.0962e+00,\n",
       "         -4.2487e-01,  1.7042e+00, -2.3838e-02, -1.9400e+00,  1.8994e+00,\n",
       "          9.9778e-01, -7.1998e-02],\n",
       "        [ 1.9789e-01, -2.4748e+00, -1.6237e+00, -1.6294e+00, -8.4048e-01,\n",
       "         -1.6976e+00,  3.2045e-01, -1.2208e+00,  7.8669e-02, -2.4554e+00,\n",
       "         -1.5855e+00, -2.4433e-01, -2.9525e+00, -6.8506e-01, -1.2567e+00,\n",
       "         -9.8907e-01, -2.3933e-01, -2.7144e+00, -4.3760e-01,  2.7612e+00,\n",
       "          5.7253e+00,  4.3492e-01,  1.3911e+00,  1.4317e+00,  9.8759e-01,\n",
       "         -3.7518e-01,  1.7328e+00,  5.5827e-02, -2.1315e+00,  1.9672e+00,\n",
       "          9.0876e-01, -1.1707e-01],\n",
       "        [-2.4677e-01, -2.3952e+00, -1.9123e+00, -1.6001e+00, -4.3182e-01,\n",
       "         -1.6250e+00,  1.2940e-01, -2.0599e+00,  1.6496e-01, -1.6751e+00,\n",
       "         -1.5387e+00,  5.0719e-01, -3.2316e+00, -7.5842e-01, -1.2665e+00,\n",
       "         -6.7307e-01, -3.2617e-01, -2.1383e+00, -9.1216e-01,  2.8298e+00,\n",
       "          6.0387e+00,  4.1624e-01,  1.7007e+00,  1.4487e+00,  6.9897e-02,\n",
       "         -5.1323e-01,  1.7014e+00,  5.9446e-01, -1.8756e+00,  1.3530e+00,\n",
       "          8.0649e-01, -6.3344e-02],\n",
       "        [-3.6620e-02, -2.3878e+00, -1.7792e+00, -1.6268e+00, -5.8025e-01,\n",
       "         -1.6995e+00,  2.1171e-01, -1.7110e+00,  2.0963e-01, -2.0522e+00,\n",
       "         -1.5399e+00,  1.0170e-01, -3.0524e+00, -6.6099e-01, -1.3249e+00,\n",
       "         -7.8866e-01, -3.1508e-01, -2.5064e+00, -7.4090e-01,  2.7787e+00,\n",
       "          5.8859e+00,  5.6409e-01,  1.4969e+00,  1.4461e+00,  7.0755e-01,\n",
       "         -4.4907e-01,  1.7301e+00,  2.3662e-01, -1.9681e+00,  1.6916e+00,\n",
       "          9.1777e-01, -7.2991e-02],\n",
       "        [-3.1024e-01, -2.2344e+00, -1.8513e+00, -1.5197e+00, -3.4335e-01,\n",
       "         -1.7393e+00,  1.2650e-01, -1.9332e+00,  5.3938e-01, -1.7526e+00,\n",
       "         -1.5088e+00,  2.7673e-02, -2.9690e+00, -6.4531e-01, -1.5267e+00,\n",
       "         -6.0702e-01, -3.1841e-01, -2.5718e+00, -1.0046e+00,  2.6130e+00,\n",
       "          5.8801e+00,  8.9712e-01,  1.5970e+00,  1.5245e+00,  1.1052e+00,\n",
       "         -4.5272e-01,  1.7560e+00,  1.1083e-01, -1.8104e+00,  1.6922e+00,\n",
       "          9.2136e-01, -3.7064e-02],\n",
       "        [-3.3842e-01, -2.2494e+00, -1.8337e+00, -1.4980e+00, -3.3037e-01,\n",
       "         -1.7337e+00,  1.2987e-01, -1.9537e+00,  5.3979e-01, -1.6866e+00,\n",
       "         -1.5178e+00,  9.6197e-02, -3.0043e+00, -6.9175e-01, -1.5247e+00,\n",
       "         -6.2197e-01, -3.2611e-01, -2.4918e+00, -1.0578e+00,  2.6292e+00,\n",
       "          5.9076e+00,  8.6479e-01,  1.6339e+00,  1.5317e+00,  1.0052e+00,\n",
       "         -4.6004e-01,  1.7628e+00,  1.9182e-01, -1.7955e+00,  1.6126e+00,\n",
       "          8.8125e-01, -3.4027e-02]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_weighted_embeddings_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d0c008a3-6535-4d76-825e-9779c0999d91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Verify if this makes sense\n",
    "context_weighted_embeddings_param_3 = (\n",
    "    attn_weights_param_norm[3, 0] * v[0]\n",
    "    + attn_weights_param_norm[3, 1] * v[1]\n",
    "    + attn_weights_param_norm[3, 2] * v[2]\n",
    "    + attn_weights_param_norm[3, 3] * v[3]\n",
    "    + attn_weights_param_norm[3, 4] * v[4]\n",
    "    + attn_weights_param_norm[3, 5] * v[5]\n",
    "    + attn_weights_param_norm[3, 6] * v[6]\n",
    "    + attn_weights_param_norm[3, 7] * v[7]\n",
    "    + attn_weights_param_norm[3, 8] * v[8]\n",
    "    #+ attn_weights_param_norm[3, 9] * v[9]\n",
    ")\n",
    "\n",
    "torch.allclose(\n",
    "    context_weighted_embeddings_param_3, context_weighted_embeddings_param[3]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7608024-00fa-4a10-97e3-b464ad29bc97",
   "metadata": {},
   "source": [
    "### Multi-Head Attention\n",
    "\n",
    "\n",
    "In what follows we demonstrate how use the parametrised attention mechanism sketched out above to develop the multi-head attention block that forms the foundation of all transformer architectures. Our aim here is purely didactic - the functions defined below won't yield anything you can train (refer to the fullcodebase in the `modelling` directory for this), but they do demonstrate how these algorithm are composed.\n",
    "\n",
    "We start by encapsulating the parametrised attention mechanism within a single function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f50eebcd-d6d7-4d22-825b-5c4062f3a254",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9, 32])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def attention(\n",
    "    query: torch.Tensor,\n",
    "    keys: torch.Tensor,\n",
    "    values: torch.Tensor,\n",
    "    causal_masking: bool = False,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Compute single attention head.\"\"\"\n",
    "    n_tokens, embedding_dim = query.shape\n",
    "    attn_weights = torch.matmul(query, keys.T) / math.sqrt(EMBEDDING_DIM)\n",
    "    if causal_masking:\n",
    "        mask = torch.triu(torch.full((n_tokens, n_tokens), True), diagonal=1)\n",
    "        attn_weights = attn_weights.masked_fill(mask, -1e10)\n",
    "    attn_weights_norm = attn_weights.softmax(dim=1)\n",
    "    context_weighted_embeddings = torch.matmul(attn_weights_norm, values)\n",
    "    return context_weighted_embeddings\n",
    "\n",
    "\n",
    "attn_head_out = attention(q, k, v)\n",
    "attn_head_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2d461599-1037-4072-b237-6129474d9598",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First you defined a single head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "59ea32d2-d6ab-4987-8434-bf6dad7f422d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of heads: 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([9, 32])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def multi_head_attention(\n",
    "    x_q: torch.Tensor,\n",
    "    x_k: torch.Tensor,\n",
    "    x_v: torch.Tensor,\n",
    "    n_heads: int,\n",
    "    causal_masking: bool = False,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Computing attention with multiple heads.\"\"\"\n",
    "    n_tokens, embedding_dim = embedded_tokens.shape\n",
    "    \n",
    "    u_q = torch.rand(n_heads, n_tokens, n_tokens)\n",
    "    u_k = torch.rand(n_heads, n_tokens, n_tokens)\n",
    "    u_v = torch.rand(n_heads, n_tokens, n_tokens)\n",
    "\n",
    "    attn_head_outputs = torch.concat(\n",
    "        [attention(u_q[h] @ x_q, u_k[h] @ x_k, u_v[h] @ x_v) for h in range(n_heads)],\n",
    "        dim=1,\n",
    "    )\n",
    "    print(\"Number of heads:\",n_heads)\n",
    "    w_out = torch.rand(n_heads * embedding_dim, embedding_dim, requires_grad=True)\n",
    "    return torch.matmul(attn_head_outputs, w_out)\n",
    "\n",
    "\n",
    "multi_head_attn_out = multi_head_attention(\n",
    "    embedded_tokens, embedded_tokens, embedded_tokens, n_heads=3\n",
    ")\n",
    "multi_head_attn_out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eac071c-48fe-48c2-ae45-08d2e65af7f5",
   "metadata": {},
   "source": [
    "# Basic Transformer Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2846a449-cde5-4d9c-b9a4-5e10d3e73bd6",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "aa3638a3-913e-4e10-9c8f-a387ccd7bbc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of heads: 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([9, 32])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def transformer_encoder_layer(\n",
    "    src_embedding: torch.Tensor, n_heads: int, causal_masking: bool = False\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Transformer encoder layer.\"\"\"\n",
    "    x = multi_head_attention(src_embedding, src_embedding, src_embedding, n_heads)\n",
    "    x = F.layer_norm(x + src_embedding, x.shape)\n",
    "\n",
    "    linear_1 = nn.Linear(EMBEDDING_DIM, 2 * EMBEDDING_DIM)\n",
    "    linear_2 = nn.Linear(2 * EMBEDDING_DIM, EMBEDDING_DIM)\n",
    "\n",
    "    x = x + F.relu(linear_2(linear_1(x)))\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "encoder_output = transformer_encoder_layer(embedded_tokens, n_heads=2)\n",
    "encoder_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2f4a0261-a0fe-41bc-9caa-627b0caaf580",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 8.1203e-01,  1.4891e-01,  1.5372e-01,  1.7760e+00, -1.9895e-01,\n",
       "         -1.0871e+00,  3.2016e-01,  1.5911e+00,  6.1307e-01, -9.1132e-02,\n",
       "          1.2693e+00,  2.0609e+00, -1.6255e-01, -9.4107e-01,  8.0400e-01,\n",
       "         -2.3842e-01, -1.4956e+00, -1.8692e-01,  8.5274e-01, -2.7596e-01,\n",
       "         -2.3209e-02, -2.0179e-01, -6.9814e-01,  1.0676e+00, -2.7210e-02,\n",
       "         -2.0933e-01, -2.1900e+00,  1.5520e+00, -3.0699e-01, -2.1589e+00,\n",
       "          2.8190e-01,  1.1620e+00],\n",
       "        [ 7.8840e-01,  5.0948e-01,  1.2440e+00,  2.0739e+00,  7.9884e-01,\n",
       "         -1.1265e+00,  7.4645e-01,  1.9345e+00,  1.2934e+00,  4.0042e-02,\n",
       "          1.5439e+00,  2.4587e+00,  5.3878e-01, -2.5842e-01,  6.7901e-01,\n",
       "          1.3712e-01, -7.1563e-01, -8.1934e-01, -6.8352e-02,  8.6518e-01,\n",
       "          5.2931e-01,  4.3073e-01, -4.7354e-01,  1.2107e+00,  9.2599e-01,\n",
       "         -4.2979e-01, -1.2033e+00,  1.6822e+00, -4.9745e-01, -1.7844e+00,\n",
       "          1.0754e+00,  1.7102e+00],\n",
       "        [ 1.0547e+00, -4.1880e-01,  1.8330e-01,  7.9888e-01, -8.5208e-02,\n",
       "         -1.2389e+00,  4.1600e-02,  6.1009e-01,  6.9393e-03, -1.5705e+00,\n",
       "          5.8571e-01,  1.6446e+00, -5.3099e-01, -1.1620e+00,  5.1224e-01,\n",
       "         -9.4771e-01, -1.4172e+00, -6.4887e-01,  5.6460e-01, -8.6856e-01,\n",
       "         -4.3394e-01,  6.6416e-02, -1.5794e+00,  1.2567e-01,  3.1958e-01,\n",
       "         -2.9039e-01, -2.5843e+00,  1.0408e+00, -8.5666e-02, -1.7060e+00,\n",
       "          4.4167e-01,  7.2389e-01],\n",
       "        [ 7.3320e-01, -3.7856e-01, -5.4633e-02,  1.4483e+00,  2.6963e-01,\n",
       "         -1.6201e+00,  4.0934e-01,  7.4346e-01,  8.1344e-01, -7.2208e-01,\n",
       "          1.0551e+00,  1.6467e+00, -2.7436e-01, -9.8356e-01,  1.1886e-02,\n",
       "         -5.3697e-01, -1.7292e+00, -6.9962e-01, -4.1741e-01, -7.7698e-01,\n",
       "         -5.0052e-01,  4.7566e-01, -1.3835e+00,  6.0497e-01,  3.5413e-01,\n",
       "         -5.6996e-01, -2.5291e+00,  7.7671e-01, -8.3337e-02, -2.0393e+00,\n",
       "          2.8900e-01,  8.8700e-01],\n",
       "        [ 4.7280e-01,  2.2791e-03, -6.4028e-01,  1.2847e+00,  5.7748e-01,\n",
       "         -1.3862e+00,  2.0514e-02,  5.8549e-01,  3.1494e-01, -5.0356e-01,\n",
       "          1.1264e+00,  1.6250e+00, -3.8151e-01, -4.7035e-01,  3.7292e-01,\n",
       "         -1.7889e-01, -1.2546e+00, -1.4479e+00,  1.0809e-01, -7.7062e-01,\n",
       "         -6.7431e-02, -3.5469e-02, -1.2052e+00,  6.5697e-01,  2.3581e-01,\n",
       "         -4.7109e-01, -2.2811e+00,  1.1149e+00, -8.8704e-01, -1.5901e+00,\n",
       "          9.2263e-01,  1.0089e+00],\n",
       "        [ 9.7503e-01,  1.2188e+00,  9.7334e-01,  1.6940e+00,  9.0632e-01,\n",
       "         -1.0439e+00,  2.1467e-01,  1.2704e+00,  1.1568e+00,  3.1283e-01,\n",
       "          1.9851e+00,  3.0940e+00,  6.3179e-01,  2.5986e-01,  7.2678e-01,\n",
       "          7.4717e-01, -8.0864e-01, -7.7244e-01,  1.7806e-01,  9.2996e-01,\n",
       "          9.8516e-01,  9.9365e-01, -8.6357e-01,  1.1705e+00,  8.4946e-01,\n",
       "         -4.4066e-01, -1.1158e+00,  1.6410e+00, -1.3064e-01, -1.9728e+00,\n",
       "          1.4288e+00,  1.9705e+00],\n",
       "        [ 6.9741e-01, -8.3391e-01,  4.5204e-02,  1.0496e+00, -4.4373e-01,\n",
       "         -1.1531e+00,  5.1903e-01,  3.4518e-01,  6.0591e-01, -1.3773e+00,\n",
       "          9.0998e-01,  1.9078e+00, -3.3447e-01, -6.0324e-01,  1.2524e+00,\n",
       "          3.2041e-02, -2.0261e+00, -7.4091e-01,  6.3850e-01, -2.8635e-01,\n",
       "         -3.2822e-01, -3.5512e-01, -5.8041e-01,  5.5733e-01, -3.1818e-01,\n",
       "         -1.9016e-01, -2.3214e+00,  9.3508e-01, -2.7109e-01, -1.7589e+00,\n",
       "          7.2432e-01,  8.0730e-01],\n",
       "        [ 2.5665e-01, -4.4064e-02, -3.5313e-01,  1.1037e+00, -9.2545e-02,\n",
       "         -1.0035e+00, -2.9278e-01,  3.6991e-01,  3.4578e-01, -6.5140e-01,\n",
       "          8.5597e-01,  1.6616e+00, -6.4857e-02, -1.0158e+00,  1.8257e-01,\n",
       "          5.1561e-03, -1.2052e+00, -1.1300e+00,  5.0876e-01, -5.4606e-01,\n",
       "          7.2437e-02,  7.7682e-02, -1.1183e+00,  5.3127e-01,  7.3712e-01,\n",
       "         -3.8760e-01, -2.3258e+00,  9.2593e-01,  1.6642e-02, -1.8245e+00,\n",
       "          6.2113e-01,  9.4416e-01],\n",
       "        [ 7.4053e-01,  5.0250e-01,  7.1275e-01,  1.6269e+00,  7.6005e-01,\n",
       "         -1.1618e+00,  2.0423e-01,  1.1280e+00,  5.5269e-01, -2.2273e-01,\n",
       "          1.7154e+00,  2.8640e+00,  5.0744e-01,  9.7890e-02,  3.1150e-01,\n",
       "          2.8462e-01, -5.4465e-01, -9.0590e-01,  4.0375e-01,  6.6797e-01,\n",
       "          3.4596e-01,  8.0237e-02, -1.0035e+00,  7.0669e-01,  2.0112e-01,\n",
       "         -5.5398e-01, -1.6768e+00,  1.3628e+00,  8.2371e-02, -1.9598e+00,\n",
       "          9.1325e-01,  1.7818e+00]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0da819a0-3569-4944-b6bf-4f79a5405779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of heads: 2\n",
      "Number of heads: 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([9, 32])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def transformer_decoder_layer(\n",
    "    src_embedding: torch.Tensor,\n",
    "    target_embedding: torch.Tensor,\n",
    "    n_heads: int,\n",
    "    causal_masking: bool = False,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Transformer decoder layer.\"\"\"\n",
    "    x = multi_head_attention(\n",
    "        target_embedding, target_embedding, target_embedding, n_heads\n",
    "    )\n",
    "    x = F.layer_norm(x + target_embedding, x.shape)\n",
    "    x = x + multi_head_attention(src_embedding, src_embedding, x, n_heads)\n",
    "    x = F.layer_norm(x, x.shape)\n",
    "\n",
    "    linear_1 = nn.Linear(EMBEDDING_DIM, 2 * EMBEDDING_DIM)\n",
    "    linear_2 = nn.Linear(2 * EMBEDDING_DIM, EMBEDDING_DIM)\n",
    "\n",
    "    x = x + F.relu(linear_2(linear_1(x)))\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "decoder_output = transformer_decoder_layer(embedded_tokens, embedded_tokens, n_heads=2)\n",
    "decoder_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4319eb-e961-464d-81e1-aaaf3f17263a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a61918-86b1-444a-8cac-2d01c725481c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d77db4-2c7a-4174-ab57-64637c1cad54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
