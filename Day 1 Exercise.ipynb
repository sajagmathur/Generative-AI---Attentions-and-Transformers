{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe035dec",
   "metadata": {},
   "source": [
    "# Lab Exercise: Computing Attention Weights and Validating Matrix Multiplication\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3a4310",
   "metadata": {},
   "source": [
    "# Setup\n",
    "Assume you have n_tokens tokens and each token is embedded into a vector of dimension embedding_dim.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9230b36d-4194-4f84-b728-eadb39b42968",
   "metadata": {},
   "source": [
    "You have a tensor embedded_tokens of shape (n_tokens, embedding_dim) containing these embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff69097b-25fe-4bed-a313-2881704d6207",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4d66cba0",
   "metadata": {},
   "source": [
    "# Task 1: Initialize an empty tensor for attention weights\n",
    "Use torch.empty() to create an empty tensor of shape (n_tokens, n_tokens) called attn_weights.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b92bbc",
   "metadata": {},
   "source": [
    "# Task 2: Compute attention weights using nested loops\n",
    "Use two for loops over the indices i and j ranging from 0 to n_tokens - 1.\n",
    "\n",
    "For each pair (i, j), compute the dot product between the embedding vectors embedded_tokens[i] and embedded_tokens[j].\n",
    "\n",
    "Store the result in attn_weights[i, j].\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e081d734",
   "metadata": {},
   "source": [
    "# Task 3: Compute attention weights using matrix multiplication\n",
    "Use the matrix multiplication operator to compute attn_weights_matmul as the product of embedded_tokens and the transpose of embedded_tokens.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a80b8b0",
   "metadata": {},
   "source": [
    "# Task 4: Verify that both methods give nearly the same results\n",
    "Use torch.allclose() to compare attn_weights and attn_weights_matmul within a tolerance (e.g., atol=1e-6).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fcca34",
   "metadata": {},
   "source": [
    "# Print whether the two matrices are approximately equal or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba8a7f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup \n",
    "\n",
    "import math\n",
    "\n",
    "#we use pytorch when parameters require complex cmputations. Helps in neural networks\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d237638-b4ee-40a0-9720-718bb8437098",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[9.7444e-27, 1.6774e-42, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Give an input data\n",
    "sentence = \"My name is Sajag mathur. He likes classical music.\"     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3cef8fa-3b55-4ddc-9856-3ef1647ee155",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1249, 19363, 15081,  9622,  4490, 13983,  3181,   955, 17675])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize(text: str, vocab_size: int) -> torch.Tensor:\n",
    "    \"\"\"Dummy text tokenizer.\"\"\"\n",
    "    #break into chunks\n",
    "    words = text.split(\" \")\n",
    "    #give integer value to each chunk - this is for testing. In case you want consistant values, you replace by huggingface transformer and integer values\n",
    "    return torch.randint(0, vocab_size, [len(words)])\n",
    "\n",
    "#give a hyperparameter called vocab_size --> this means that total number of unique tokens that model can recognize and process is 20000.\n",
    "VOCAB_SIZE = 20000\n",
    "\n",
    "tokenized_sentence = tokenize(sentence, VOCAB_SIZE)\n",
    "n_tokens = len(tokenized_sentence)\n",
    "tokenized_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3c80eef-eead-452e-92f3-e05c3fdd0da5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9, 32])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Generate Embeddings and convert input tokens into vectors --> This is run through neural network.embedding\n",
    "# Vocabulary Size and Dimension of Embeddings --> you tokenize the sentence\n",
    "EMBEDDING_DIM = 32\n",
    "\n",
    "embedding_layer = nn.Embedding(VOCAB_SIZE, EMBEDDING_DIM)\n",
    "embedded_tokens = embedding_layer(tokenized_sentence)\n",
    "embedded_tokens.shape\n",
    "\n",
    "#Torch.size(size of input data, embedding dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49f4b3d1-da13-4929-8d77-87e6b232d2bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9, 9])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#for all tokens --> attention weight is dot product of each embedding token against any other embedding token\n",
    "\n",
    "attn_weights = torch.empty(n_tokens, n_tokens)\n",
    "\n",
    "\n",
    "for i in range(n_tokens):\n",
    "    for j in range(n_tokens):\n",
    "        attn_weights[i, j] = torch.dot(embedded_tokens[i], embedded_tokens[j])\n",
    "\n",
    "attn_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e6d223b-429d-4574-988e-b49d7e7fe62b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.4985e+01,  1.2610e+01,  2.7600e+00, -2.5685e-01, -2.1084e+00,\n",
       "          2.9533e+00, -3.7966e+00,  5.4972e+00,  5.6959e+00],\n",
       "        [ 1.2610e+01,  3.7651e+01,  2.9662e+00, -8.0765e-01, -9.3061e+00,\n",
       "         -2.9145e+00,  4.3191e+00, -1.8725e+00,  1.0776e+01],\n",
       "        [ 2.7600e+00,  2.9662e+00,  1.7985e+01,  2.0308e+00, -8.3572e+00,\n",
       "         -7.7152e-01, -1.1566e+00,  8.1203e+00,  5.8534e-01],\n",
       "        [-2.5685e-01, -8.0765e-01,  2.0308e+00,  2.7836e+01,  3.3133e-01,\n",
       "         -6.4917e+00,  4.7592e+00,  7.7281e-01,  3.0164e+00],\n",
       "        [-2.1084e+00, -9.3061e+00, -8.3572e+00,  3.3133e-01,  3.2199e+01,\n",
       "         -6.0480e+00,  1.8887e+00, -6.9616e+00,  5.9405e+00],\n",
       "        [ 2.9533e+00, -2.9145e+00, -7.7152e-01, -6.4917e+00, -6.0480e+00,\n",
       "          2.3370e+01, -4.6410e+00, -5.3984e+00, -3.2613e-02],\n",
       "        [-3.7966e+00,  4.3191e+00, -1.1566e+00,  4.7592e+00,  1.8887e+00,\n",
       "         -4.6410e+00,  3.7193e+01, -5.4652e+00,  9.4490e+00],\n",
       "        [ 5.4972e+00, -1.8725e+00,  8.1203e+00,  7.7281e-01, -6.9616e+00,\n",
       "         -5.3984e+00, -5.4652e+00,  2.8965e+01, -2.1343e+00],\n",
       "        [ 5.6959e+00,  1.0776e+01,  5.8534e-01,  3.0164e+00,  5.9405e+00,\n",
       "         -3.2613e-02,  9.4490e+00, -2.1343e+00,  2.5205e+01]],\n",
       "       grad_fn=<CopySlices>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7825760-d3b7-4e79-a382-9054c135a870",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_weights_matmul = torch.matmul(embedded_tokens, embedded_tokens.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "074887f9-47a7-4ada-b70f-ce57315ad594",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.4985e+01,  1.2610e+01,  2.7600e+00, -2.5685e-01, -2.1084e+00,\n",
       "          2.9533e+00, -3.7966e+00,  5.4972e+00,  5.6959e+00],\n",
       "        [ 1.2610e+01,  3.7651e+01,  2.9662e+00, -8.0765e-01, -9.3061e+00,\n",
       "         -2.9145e+00,  4.3191e+00, -1.8725e+00,  1.0776e+01],\n",
       "        [ 2.7600e+00,  2.9662e+00,  1.7985e+01,  2.0308e+00, -8.3572e+00,\n",
       "         -7.7152e-01, -1.1566e+00,  8.1203e+00,  5.8534e-01],\n",
       "        [-2.5685e-01, -8.0765e-01,  2.0308e+00,  2.7836e+01,  3.3133e-01,\n",
       "         -6.4917e+00,  4.7592e+00,  7.7281e-01,  3.0164e+00],\n",
       "        [-2.1084e+00, -9.3061e+00, -8.3572e+00,  3.3133e-01,  3.2199e+01,\n",
       "         -6.0480e+00,  1.8887e+00, -6.9616e+00,  5.9405e+00],\n",
       "        [ 2.9533e+00, -2.9145e+00, -7.7152e-01, -6.4917e+00, -6.0480e+00,\n",
       "          2.3370e+01, -4.6410e+00, -5.3984e+00, -3.2613e-02],\n",
       "        [-3.7966e+00,  4.3191e+00, -1.1566e+00,  4.7592e+00,  1.8887e+00,\n",
       "         -4.6410e+00,  3.7193e+01, -5.4652e+00,  9.4490e+00],\n",
       "        [ 5.4972e+00, -1.8725e+00,  8.1203e+00,  7.7281e-01, -6.9616e+00,\n",
       "         -5.3984e+00, -5.4652e+00,  2.8965e+01, -2.1343e+00],\n",
       "        [ 5.6959e+00,  1.0776e+01,  5.8534e-01,  3.0164e+00,  5.9405e+00,\n",
       "         -3.2613e-02,  9.4490e+00, -2.1343e+00,  2.5205e+01]],\n",
       "       grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weights_matmul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d386bf4-87cc-4afb-9edd-a9d67062d35f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check if both approaches are same or not\n",
    "torch.allclose(attn_weights_matmul, attn_weights, atol=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe2ddcf-db3b-4a03-b5bb-f5fed405a812",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
