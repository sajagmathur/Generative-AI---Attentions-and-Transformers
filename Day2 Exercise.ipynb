{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45cf307d-0ef5-409a-876f-83effc10918a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "# For better display in notebooks\n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27831149-3ab0-4b47-bcb3-a5f8e1df23e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**`embedded_tokens` (simulated input/value vectors) Shape:** `torch.Size([3, 4])`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.1000, 0.2000, 0.3000, 0.4000],\n",
       "        [0.5000, 0.6000, 0.7000, 0.8000],\n",
       "        [0.9000, 0.0000, 0.1000, 0.2000]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define embedding dimension (dk)\n",
    "EMBEDDING_DIM = 4\n",
    "\n",
    "# Define sequence length (number of tokens/words)\n",
    "SEQUENCE_LENGTH = 3\n",
    "\n",
    "# --- Simulate Input Embeddings (Our \"Value\" vectors initially) ---\n",
    "# In a real model, these would come from an embedding layer\n",
    "# or previous transformer block.\n",
    "# Let's represent 5 tokens, each with an embedding of 4 dimensions.\n",
    "# Shape: (sequence_length, embedding_dim)\n",
    "embedded_tokens = torch.tensor([\n",
    "    [0.1, 0.2, 0.3, 0.4], # Token 0: \"He\"\n",
    "    [0.5, 0.6, 0.7, 0.8], # Token 1: \"is\"\n",
    "    [0.9, 0.0, 0.1, 0.2], # Token 2: \"awesome\"\n",
    "], dtype=torch.float32)\n",
    "\n",
    "display(Markdown(f\"**`embedded_tokens` (simulated input/value vectors) Shape:** `{embedded_tokens.shape}`\"))\n",
    "display(embedded_tokens)\n",
    "print(\"\\n---\\n\")\n",
    "\n",
    "# --- Simulate Query and Key Matrices ---\n",
    "# In self-attention, Q, K, V typically come from linear transformations\n",
    "# of the same input embeddings. For simplicity, let's just make Q, K same as V for now.\n",
    "# In a real scenario, you'd have:\n",
    "# Q = embedded_tokens @ W_q\n",
    "# K = embedded_tokens @ W_k\n",
    "# V = embedded_tokens @ W_v\n",
    "# where W_q, W_k, W_v are weight matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "251808da-b8c7-46c0-82b0-ffffa880d928",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**`queries` Shape:** `torch.Size([3, 4])`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.1000, 0.2000, 0.3000, 0.4000],\n",
       "        [0.5000, 0.6000, 0.7000, 0.8000],\n",
       "        [0.9000, 0.0000, 0.1000, 0.2000]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**`keys` Shape:** `torch.Size([3, 4])`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.1000, 0.2000, 0.3000, 0.4000],\n",
       "        [0.5000, 0.6000, 0.7000, 0.8000],\n",
       "        [0.9000, 0.0000, 0.1000, 0.2000]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**`values` (same as embedded_tokens for this example) Shape:** `torch.Size([3, 4])`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.1000, 0.2000, 0.3000, 0.4000],\n",
       "        [0.5000, 0.6000, 0.7000, 0.8000],\n",
       "        [0.9000, 0.0000, 0.1000, 0.2000]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# For this example, let's make Q and K identical to embedded_tokens\n",
    "# to demonstrate self-attention where each token attends to all others including itself.\n",
    "queries = embedded_tokens\n",
    "keys = embedded_tokens\n",
    "values = embedded_tokens # Renaming for clarity as per Q,K,V convention\n",
    "\n",
    "display(Markdown(f\"**`queries` Shape:** `{queries.shape}`\"))\n",
    "display(queries)\n",
    "display(Markdown(f\"**`keys` Shape:** `{keys.shape}`\"))\n",
    "display(keys)\n",
    "display(Markdown(f\"**`values` (same as embedded_tokens for this example) Shape:** `{values.shape}`\"))\n",
    "display(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1a8ffb9-7059-4582-8738-376d134afefc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3000, 0.7000, 0.2000],\n",
       "        [0.7000, 1.7400, 0.6800],\n",
       "        [0.2000, 0.6800, 0.8600]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# QK^T\n",
    "# queries shape: (seq_len, embedding_dim)\n",
    "# keys.T shape: (embedding_dim, seq_len)\n",
    "# Resulting attn_weights shape: (seq_len, seq_len)\n",
    "# Element (i, j) will be the dot product of query_i and key_j\n",
    "attn_weights = torch.matmul(queries, keys.transpose(-2, -1))\n",
    "attn_weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95011f1d-76b6-4ea6-a038-d8605a77e53d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Raw Attention Weights ($QK^T$) Shape:** `torch.Size([3, 3])`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.3000, 0.7000, 0.2000],\n",
       "        [0.7000, 1.7400, 0.6800],\n",
       "        [0.2000, 0.6800, 0.8600]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Each row corresponds to a Query, and each column to a Key."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "For example, `attn_weights[0, 1]` is the similarity between Query 0 and Key 1."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(f\"**Raw Attention Weights ($QK^T$) Shape:** `{attn_weights.shape}`\"))\n",
    "display(attn_weights)\n",
    "print(\"\\n---\")\n",
    "display(Markdown(\"Each row corresponds to a Query, and each column to a Key.\"))\n",
    "display(Markdown(\"For example, `attn_weights[0, 1]` is the similarity between Query 0 and Key 1.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f6212fa-b1be-40a5-ae2f-04cd3d9f01cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:5: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:5: SyntaxWarning: invalid escape sequence '\\s'\n",
      "C:\\Users\\sajag177350\\AppData\\Local\\Temp\\1\\ipykernel_29892\\2532459264.py:5: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  display(Markdown(f\"**Scaling Factor ($\\sqrt{{d_k}}$):** `{scaling_factor:.4f}`\"))\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Scaling Factor ($\\sqrt{d_k}$):** `2.0000`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Scaled Attention Weights Shape:** `torch.Size([3, 3])`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.1500, 0.3500, 0.1000],\n",
       "        [0.3500, 0.8700, 0.3400],\n",
       "        [0.1000, 0.3400, 0.4300]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Notice how the values are now smaller, which helps prevent vanishing gradients after softmax."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Scaling\n",
    "scaling_factor = math.sqrt(EMBEDDING_DIM)\n",
    "attn_weights_scaled = attn_weights / scaling_factor\n",
    "\n",
    "display(Markdown(f\"**Scaling Factor ($\\sqrt{{d_k}}$):** `{scaling_factor:.4f}`\"))\n",
    "display(Markdown(f\"**Scaled Attention Weights Shape:** `{attn_weights_scaled.shape}`\"))\n",
    "display(attn_weights_scaled)\n",
    "print(\"\\n---\")\n",
    "display(Markdown(\"Notice how the values are now smaller, which helps prevent vanishing gradients after softmax.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5048032f-0cbe-4d72-b801-0c87c1aef431",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Normalized Attention Weights (Softmax Output) Shape:** `torch.Size([3, 3])`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.3152, 0.3850, 0.2998],\n",
       "        [0.2723, 0.4581, 0.2696],\n",
       "        [0.2731, 0.3471, 0.3798]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Each row now represents a probability distribution. Let's check a row's sum:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of first row: 1.0000\n",
      "Sum of second row: 1.0000\n",
      "Sum of third row: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Apply softmax along the last dimension (dim=1 for a 2D tensor of (queries, keys))\n",
    "# This ensures that for each query (row), the attention weights across all keys (columns) sum to 1.\n",
    "attn_weights_norm = F.softmax(attn_weights_scaled, dim=1)\n",
    "\n",
    "display(Markdown(f\"**Normalized Attention Weights (Softmax Output) Shape:** `{attn_weights_norm.shape}`\"))\n",
    "display(attn_weights_norm)\n",
    "print(\"\\n---\")\n",
    "display(Markdown(\"Each row now represents a probability distribution. Let's check a row's sum:\"))\n",
    "print(f\"Sum of first row: {attn_weights_norm[0].sum().item():.4f}\")\n",
    "print(f\"Sum of second row: {attn_weights_norm[1].sum().item():.4f}\")\n",
    "print(f\"Sum of third row: {attn_weights_norm[2].sum().item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc8cca7e-aabe-4044-9c06-c3c89e327158",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3152, 0.3850, 0.2998],\n",
       "        [0.2723, 0.4581, 0.2696],\n",
       "        [0.2731, 0.3471, 0.3798]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weights_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f3fb1f09-1d52-4930-8846-3bdfc6c0a02a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Context-Weighted Embeddings Shape:** `torch.Size([3, 4])`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.4939, 0.2940, 0.3940, 0.4940],\n",
       "        [0.4989, 0.3293, 0.4293, 0.5293],\n",
       "        [0.5427, 0.2629, 0.3629, 0.4629]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Each row is a new, context-aware representation for the corresponding input token."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# attn_weights_norm shape: (num_queries, num_keys) -> (SEQUENCE_LENGTH, SEQUENCE_LENGTH)\n",
    "# values shape: (num_keys, embedding_dim) -> (SEQUENCE_LENGTH, EMBEDDING_DIM)\n",
    "# Resulting context_weighted_embeddings shape: (num_queries, embedding_dim)\n",
    "context_weighted_embeddings = torch.matmul(attn_weights_norm, values)\n",
    "\n",
    "display(Markdown(f\"**Context-Weighted Embeddings Shape:** `{context_weighted_embeddings.shape}`\"))\n",
    "display(context_weighted_embeddings)\n",
    "print(\"\\n---\")\n",
    "display(Markdown(\"Each row is a new, context-aware representation for the corresponding input token.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b018cfef-7390-4c2c-b284-e241938e850d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Manual Calculation for 4th Query (Index 2):**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The 4th query's attention weights are: `tensor([0.2731, 0.3471, 0.3798])`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The original value vectors (`embedded_tokens` are: "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.1000, 0.2000, 0.3000, 0.4000],\n",
       "        [0.5000, 0.6000, 0.7000, 0.8000],\n",
       "        [0.9000, 0.0000, 0.1000, 0.2000]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(f\"**Manual Calculation for 4th Query (Index 2):**\"))\n",
    "display(Markdown(f\"The 4th query's attention weights are: `{attn_weights_norm[2]}`\"))\n",
    "display(Markdown(f\"The original value vectors (`embedded_tokens` are: \"))\n",
    "display(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0364dcf9-74d9-473e-9921-4ce599767e5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Manually Calculated Context for 3rd Query:** `tensor([0.5427, 0.2629, 0.3629, 0.4629])`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**PyTorch Calculated Context for 3rd Query:** `tensor([0.5427, 0.2629, 0.3629, 0.4629])`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "context_weighted_embeddings_2_manual = (\n",
    "    attn_weights_norm[2, 0] * values[0] +\n",
    "    attn_weights_norm[2, 1] * values[1] +\n",
    "    attn_weights_norm[2, 2] * values[2]\n",
    ")\n",
    "\n",
    "display(Markdown(f\"**Manually Calculated Context for 3rd Query:** `{context_weighted_embeddings_2_manual}`\"))\n",
    "display(Markdown(f\"**PyTorch Calculated Context for 3rd Query:** `{context_weighted_embeddings[2]}`\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5c574315-df6a-4b6f-b327-1221dd1234b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The manual calculation matches the `torch.matmul` result!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check if they are approximately equal\n",
    "if torch.allclose(context_weighted_embeddings_2_manual, context_weighted_embeddings[2]):\n",
    "    display(Markdown(\"The manual calculation matches the `torch.matmul` result!\"))\n",
    "else:\n",
    "    display(Markdown(\"Mismatch detected (shouldn't happen if numbers are floats, might be tiny precision diff).\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76b0e5c-c060-4be3-a1d0-bfb05435e827",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
