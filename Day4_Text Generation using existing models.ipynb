{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3391ef9-594c-450a-8b47-09b596383c0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'blessed']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"I am blessed\" \n",
    "\n",
    "[\"I\",\"am\",\"blessed\"] #- Token of words\n",
    "\n",
    "# [1 0  0 \n",
    "# 0  1  0\n",
    "# 0  0  1] * Weight Random matrix \n",
    "\n",
    "# Take a list of token of words as a matrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34bb8ae9-3619-46aa-86bf-d4043cc3ecb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monika: [0.5391266  0.28676013 0.68882481]\n",
      "likes: [0.74886597 0.23254551 0.97288267]\n",
      "Coffee: [0.05358873 0.17295359 0.08770437]\n"
     ]
    }
   ],
   "source": [
    "## Word Emdedding using Python \n",
    "import numpy as np\n",
    "\n",
    "# Define the vocabulary\n",
    "vocab = {\"Monika\": 0, \"likes\": 1, \"Coffee\": 2}\n",
    "\n",
    "\n",
    "# One-hot encode each word - using numpy -- give index to each words\n",
    "def one_hot_encoding(word_index, vocab_size):\n",
    "    one_hot = np.zeros(vocab_size)\n",
    "    one_hot[word_index] = 1\n",
    "    return one_hot\n",
    "\n",
    "# Define the weight matrix (random initialization for simplicity)\n",
    "weight_matrix = np.random.rand(len(vocab), 3)\n",
    "\n",
    "# Multiply one-hot encoded words with the weight matrix\n",
    "word_embeddings = {}\n",
    "for word, index in vocab.items():\n",
    "    one_hot_encoded = one_hot_encoding(index, len(vocab))\n",
    "    embedding = np.dot(one_hot_encoded, weight_matrix)\n",
    "    word_embeddings[word] = embedding\n",
    "\n",
    "# Display word embeddings\n",
    "for word, embedding in word_embeddings.items():\n",
    "    print(f\"{word}: {embedding}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6415c100-f760-4d11-8b86-6f92995650c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word embeddings with position encodings:\n",
      "Monika: [0.1 1.2 0.3]\n",
      "likes: [1.04147098 0.84030231 0.10215443]\n",
      "coffee: [ 1.20929743 -0.31614684  0.20430886]\n"
     ]
    }
   ],
   "source": [
    "#Generate positional encodings using sine and cosine functions\n",
    "import numpy as np\n",
    "\n",
    "# Position Encoding\n",
    "def position_encoding(sentence_length, embedding_dim):\n",
    "    position_encodings = np.zeros((sentence_length, embedding_dim))\n",
    "    for pos in range(sentence_length):\n",
    "        for i in range(embedding_dim):\n",
    "            if i % 2 == 0:\n",
    "                position_encodings[pos, i] = np.sin(pos / (10000 ** (i / embedding_dim)))  # scale th position encoding \n",
    "            else:\n",
    "                position_encodings[pos, i] = np.cos(pos / (10000 ** ((i - 1) / embedding_dim)))\n",
    "    return position_encodings\n",
    "\n",
    "# Assuming we have word embeddings for \"Monika\", \"likes\", \"Coffee\" as follows\n",
    "word_embeddings = {\n",
    "    \"Monika\": np.array([0.1, 0.2, 0.3]),\n",
    "    \"likes\": np.array([0.2, 0.3, 0.1]),\n",
    "    \"coffee\": np.array([0.3, 0.1, 0.2])\n",
    "}\n",
    "\n",
    "# Get the position encodings\n",
    "sentence_length = 3\n",
    "embedding_dim = 3\n",
    "pos_encodings = position_encoding(sentence_length, embedding_dim)\n",
    "\n",
    "# Add position encodings to word embeddings\n",
    "for i, word in enumerate(word_embeddings):\n",
    "    word_embeddings[word] += pos_encodings[i % sentence_length]\n",
    "\n",
    "# Display word embeddings with position encodings\n",
    "print(\"Word embeddings with position encodings:\")\n",
    "for word, embedding in word_embeddings.items():\n",
    "    print(f\"{word}: {embedding}\")\n",
    "# This helps us identify which position in the sentence - we always provide sine functions since it gives the position in the sentence details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9874a028-e61b-45e6-ad07-b64576b3ecbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will Implement text generation using models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1cc7e66-d22b-4166-9d3f-9e4bc3c0334e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\sajag177350\\appdata\\local\\anaconda3\\lib\\site-packages (2.7.1+cpu)\n",
      "Requirement already satisfied: filelock in c:\\users\\sajag177350\\appdata\\local\\anaconda3\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\sajag177350\\appdata\\local\\anaconda3\\lib\\site-packages (from torch) (4.14.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\sajag177350\\appdata\\local\\anaconda3\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\sajag177350\\appdata\\local\\anaconda3\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\sajag177350\\appdata\\local\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\sajag177350\\appdata\\local\\anaconda3\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\sajag177350\\appdata\\local\\anaconda3\\lib\\site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\sajag177350\\appdata\\local\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\sajag177350\\appdata\\local\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec6dfcab-52b3-4e53-b897-c76e96ee93e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "enter your text :  I love LLMs, they make me happy!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"HF_HUB_DISABLE_SYMLINKS_WARNING\"] = \"1\"\n",
    "import torch\n",
    "from transformers import BertModel, BertTokenizer, BertConfig\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "machine_T = 'bert-base-multilingual-cased'\n",
    "tokenizer = BertTokenizer.from_pretrained(machine_T )\n",
    "model = BertModel.from_pretrained(machine_T )\n",
    "\n",
    "# Input text\n",
    "input_text = input(\"enter your text : \")\n",
    "\n",
    "# Tokenize input text\n",
    "input_ids = tokenizer.encode(input_text, add_special_tokens=True, return_tensors=\"pt\")\n",
    "\n",
    "# Get BERT model's output\n",
    "outputs = model(input_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9690d41c-42b0-4e96-b2db-b0fa2712daa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-0.3841, -0.0234, -0.2017,  ...,  0.5245, -0.1391,  0.1791],\n",
       "         [-0.1159, -0.1716,  0.2443,  ...,  0.9176, -0.9874,  0.0983],\n",
       "         [-0.0056, -0.0829, -0.4190,  ...,  0.9390, -0.7476,  0.7224],\n",
       "         ...,\n",
       "         [-0.8440, -0.4264, -0.3613,  ...,  0.8687, -0.2631,  0.2862],\n",
       "         [-0.7841, -0.0480, -0.4827,  ...,  0.4807, -0.1298,  0.2611],\n",
       "         [-0.6237, -0.0068,  0.0317,  ...,  0.3305,  0.0342,  0.0811]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[ 0.4370, -0.0656,  0.3193, -0.2134, -0.1019,  0.5360,  0.2850,  0.1132,\n",
       "         -0.3671,  0.3552, -0.0741, -0.2581, -0.2375, -0.0639,  0.1875, -0.3000,\n",
       "          0.7516,  0.0432,  0.2976, -0.3649, -1.0000, -0.1487, -0.2942, -0.2246,\n",
       "         -0.3305, -0.0301, -0.2011,  0.1315,  0.2810, -0.2672,  0.2703, -1.0000,\n",
       "          0.6721,  0.7428,  0.3939, -0.1486,  0.2582,  0.2270,  0.1492, -0.3759,\n",
       "         -0.0324,  0.0083, -0.2686,  0.2362, -0.1055, -0.2473, -0.1928,  0.3042,\n",
       "         -0.4996,  0.1299,  0.0641,  0.2835,  0.4437,  0.4458,  0.1707,  0.1106,\n",
       "          0.2313,  0.3552,  0.4212, -0.2781,  0.0407,  0.4820,  0.2932, -0.2392,\n",
       "         -0.1989, -0.4410,  0.1246, -0.0781,  0.5699, -0.3793, -0.2260, -0.4977,\n",
       "         -0.2049,  0.1773,  0.1657, -0.3667,  0.3513,  0.3529,  0.2620, -0.1954,\n",
       "         -0.3581, -0.5376, -0.5063,  0.1720, -0.0866,  0.1299,  0.3088, -0.4144,\n",
       "          0.1917, -0.0786,  0.1455,  0.5864, -0.2265,  0.3067, -0.3138, -0.2997,\n",
       "         -0.8881, -0.2499, -0.1492, -0.5334, -0.2425,  0.2407, -0.4380, -0.1175,\n",
       "         -0.2427, -0.3877,  0.2091,  0.2753, -0.1980,  0.2024,  0.3059, -0.5892,\n",
       "         -0.2195,  0.0895, -0.5153,  0.9856, -0.2980,  0.2586, -0.0733, -0.1876,\n",
       "         -0.6822,  1.0000,  0.1261, -0.3195,  0.0522,  0.0989, -0.5794,  0.1016,\n",
       "          0.2972,  0.4297,  0.0919, -0.1367, -0.3597, -0.2896, -0.8453, -0.2392,\n",
       "         -0.2636,  0.4134, -0.4601, -0.1324,  0.2278,  0.5451,  0.2161, -0.1585,\n",
       "         -0.2363, -0.2157,  0.5415, -0.1916,  1.0000,  0.7061, -0.1654, -0.1591,\n",
       "          0.5823, -0.7364, -0.2861, -0.3936, -0.2289, -0.4253,  0.3683,  0.3356,\n",
       "          0.2368, -0.0827, -0.1414, -0.2337,  0.3127, -0.7264, -0.1703,  0.2096,\n",
       "          0.1664,  0.2811, -0.1616,  0.3007,  0.4069, -0.3059, -0.2419,  0.2362,\n",
       "          0.2103, -0.1281, -0.3532, -0.1891,  0.2811, -0.1490, -0.5308,  0.0821,\n",
       "         -0.3152, -0.5367,  0.1762, -0.0988, -0.2550,  0.2377, -0.4219,  0.1634,\n",
       "         -0.3418,  0.2653,  0.3468,  0.3196, -0.5413,  0.2739,  0.3539,  0.3025,\n",
       "          0.3124,  0.2187,  0.1362,  0.2668, -0.3169, -0.7655,  0.3829,  0.1549,\n",
       "          0.5299, -0.2154, -0.3975, -0.2178,  0.6450,  0.3391, -0.2656,  0.4821,\n",
       "          0.3995, -0.3785, -0.0633,  0.3508, -0.0590, -0.4693, -0.3534, -0.2482,\n",
       "          0.0393,  0.0732,  0.1395,  0.0619,  0.2335, -0.2410, -0.0156, -0.1173,\n",
       "          0.2145,  0.4914, -0.1462,  0.8487, -0.1376,  0.1563, -0.4474, -0.0854,\n",
       "          0.2977, -0.2106,  0.1957,  0.9745,  0.3134, -0.4233,  0.2352,  0.2441,\n",
       "          0.2173, -0.2371,  0.2689, -0.5707,  0.7440,  0.4100,  0.2015, -1.0000,\n",
       "          0.1416,  0.2374,  0.3105,  0.2038,  0.2011,  0.3333,  0.1034,  0.9206,\n",
       "         -0.3994, -0.5418, -0.3930, -0.2059, -0.6306, -0.2743, -0.2736, -0.2512,\n",
       "         -0.2131, -0.0913, -0.1770,  0.1797,  0.2296, -0.9973,  0.8840,  0.1698,\n",
       "         -0.2561,  0.0755,  0.3953, -1.0000,  0.3328, -0.2518, -0.2907,  0.3273,\n",
       "         -0.5047, -0.3161,  0.2120,  0.4584,  0.3786,  0.2289,  0.1008,  0.4833,\n",
       "         -0.1180,  0.0333,  0.1895, -0.0691,  0.7092,  0.0506,  0.2859,  0.3311,\n",
       "         -0.0772,  0.3219, -0.3733,  0.4082,  0.4902,  0.2297,  0.0895, -0.2473,\n",
       "          0.3946, -0.8180,  0.2189, -0.2366, -0.1594, -0.0878,  0.2675, -0.3587,\n",
       "         -0.3200,  0.1763, -0.3895,  1.0000,  0.3091, -0.3051, -0.4200,  0.5751,\n",
       "          0.5834, -0.3616, -0.7327, -0.1731,  0.6301,  0.4440,  0.1573,  0.0491,\n",
       "         -0.0832,  0.2606, -0.2117, -0.3114, -0.0360, -0.4595,  0.3537, -0.0962,\n",
       "         -0.4788,  0.0611, -0.2645, -0.0759, -0.8627,  0.2166,  0.2320,  0.2506,\n",
       "          0.2213,  0.2737, -0.2516,  0.5661,  0.4867, -0.2925, -0.3022, -0.3949,\n",
       "         -0.4450,  0.1467, -0.2488, -0.3975,  0.2400, -0.6907,  0.1182,  0.0928,\n",
       "         -0.2513, -0.3661,  0.3381, -1.0000,  0.0130,  0.2595, -0.3678,  0.2191,\n",
       "         -0.3625, -0.2151,  0.3360,  0.3767, -0.0399,  0.1127, -0.5502,  0.3676,\n",
       "         -0.0735,  0.0261,  0.8397,  0.6211,  0.0890, -0.3088,  0.2159, -0.5955,\n",
       "         -0.1128,  0.4062,  0.2137, -0.1859,  0.1943,  0.3890,  0.0535, -0.1876,\n",
       "          0.3630, -0.2077, -0.1434,  0.2678,  0.1009, -0.2096, -0.2713,  0.3574,\n",
       "         -0.5779,  0.4067,  0.2852,  0.3085,  0.2517,  0.4538, -0.3248, -0.2187,\n",
       "         -0.1886,  0.0701, -0.4120, -0.2111, -0.1056,  1.0000,  0.4344,  0.3618,\n",
       "         -0.3512,  0.2110,  0.4308, -0.3851,  0.1879,  0.2280,  0.2214, -0.0674,\n",
       "          0.1596,  0.3765,  0.3855,  0.3553,  0.4337,  0.4427, -0.3836,  0.7594,\n",
       "         -0.3494, -0.3711, -0.9978,  0.1591,  0.3552, -0.3702, -0.6369,  0.3330,\n",
       "         -0.3230,  0.1295, -0.2513,  0.1119,  0.1339, -0.2192,  0.4317, -0.2490,\n",
       "          1.0000, -0.3163,  0.2073,  0.2983,  0.2079, -0.3353, -0.2536, -0.2498,\n",
       "          0.3967, -0.0995,  0.2674, -0.9573,  0.3571,  0.2702,  0.2843, -0.0127,\n",
       "          0.2445, -0.4268,  0.2920, -0.0097, -0.2436, -0.3683,  0.3905, -0.4736,\n",
       "          0.5811, -0.1467,  0.0955, -0.3668,  0.2004, -0.2561,  0.3151, -0.2084,\n",
       "          0.2920, -0.1059, -0.2606, -0.4351,  0.1908, -0.4752,  1.0000, -0.0750,\n",
       "          0.4148, -0.3800,  0.3034, -0.2502,  0.3613,  0.7795, -0.3804,  0.2014,\n",
       "          0.3786, -0.7637,  0.3453, -0.1649, -0.8386, -0.0729,  0.9655,  0.1147,\n",
       "          0.4051,  0.4885,  0.4059,  0.3513, -0.3127,  0.1946,  0.8895,  0.2316,\n",
       "          0.2428,  0.3466, -0.0479, -0.4491, -0.3546,  1.0000,  1.0000,  0.2226,\n",
       "          0.3534, -0.3056, -0.3709, -0.2877,  0.0534,  0.1681,  0.2920, -0.2353,\n",
       "          0.0920, -0.4520, -0.1936, -0.2485, -0.1943, -0.2357,  0.2451, -0.3105,\n",
       "          0.6405,  0.4537,  0.1267,  0.5645,  0.3337,  0.3593, -0.0415, -0.2519,\n",
       "          0.5144, -0.2844, -0.2452, -0.4530,  0.1295, -1.0000, -0.3091, -0.1011,\n",
       "         -0.2810,  0.5201,  0.1498,  0.1440, -0.3147, -0.2305, -0.3789,  0.3286,\n",
       "          0.2792,  0.0691, -0.1812, -0.3905,  0.5413, -0.4579,  0.3311, -0.3832,\n",
       "         -0.1021, -0.7104, -0.2959, -0.2962,  0.4553, -0.2360, -0.2813,  0.2305,\n",
       "          0.3884,  0.1106, -0.4059,  0.2568, -0.2422,  0.0709,  0.3982,  0.2656,\n",
       "          0.3715, -0.3725, -0.1739, -0.2786, -0.2929, -0.3289,  0.1915, -0.4046,\n",
       "          0.2511, -0.2726,  0.0590, -0.3367,  0.1421,  0.2176,  0.5049, -0.2907,\n",
       "          0.5375,  0.4223, -0.2238,  0.4638,  0.1524, -0.5064, -0.2625,  1.0000,\n",
       "          0.4326,  0.1723,  0.1612, -0.2524,  0.3743,  0.1250,  0.5914, -0.2530,\n",
       "          0.8702, -0.4080,  0.2325,  0.1648,  0.3806,  0.2127,  0.3033,  0.3224,\n",
       "          0.7815,  0.3190,  0.4163,  0.2822,  0.3506,  0.3792,  0.4375,  0.2661,\n",
       "          0.4785,  0.4745, -0.2930,  0.3228, -0.2267, -0.1313,  0.0284, -0.2044,\n",
       "         -0.4965,  0.1237, -0.1744,  0.0151, -0.3496,  0.5005, -0.1647,  0.2439,\n",
       "         -0.1232, -0.2277,  0.6459, -0.5153,  0.2984, -0.1509,  0.0038, -0.8342,\n",
       "          0.1853, -0.2612, -0.4945, -0.2434, -0.5892,  0.3036,  0.3231, -0.3229,\n",
       "          0.2315, -0.3181,  0.2684, -0.1896, -0.3203,  0.0612, -1.0000,  0.1949,\n",
       "          0.1392, -0.3440,  0.2747,  0.0442,  0.2450,  0.3762, -0.3456, -0.3479,\n",
       "         -0.1035,  0.3417, -0.3306,  0.0591,  0.1833, -0.4739, -0.1665,  0.1191,\n",
       "         -0.2038,  0.2919,  0.2825, -0.4500,  0.2693, -0.3809,  0.2870, -0.2555,\n",
       "          0.2750, -0.4657, -0.2962,  0.1849, -0.5593, -0.5272, -0.1680,  0.2808,\n",
       "         -0.2096,  0.2056,  0.2025, -0.2074,  0.3901, -0.2314,  0.2717, -0.1751,\n",
       "          0.3021, -0.8460, -0.1699, -0.5187, -0.1930,  0.2088,  0.4526,  0.2389,\n",
       "          0.3167,  0.0170,  0.1940, -0.0899,  0.2332,  0.2760, -0.1465,  0.1682,\n",
       "         -0.2937,  0.1823, -0.3858,  0.0801, -0.9915, -0.4638,  0.0299,  0.3321,\n",
       "          0.4199, -0.3243, -0.2237, -0.3525, -0.2028,  0.2532,  0.3790,  0.2496,\n",
       "          0.3984,  0.3328, -0.2131, -0.0682,  0.7743, -0.2570, -0.0783,  0.5110,\n",
       "          0.1237,  0.8680,  0.4918,  0.2984,  0.2608, -0.3012,  0.3076,  0.2694]],\n",
       "       grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c471d2c-376c-4536-81cd-b5abc97e4208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded output from positional encoder: I love LLMs, they make me happy!\n",
      "Final output: tensor([[[-0.3841, -0.0234, -0.2017,  ...,  0.5245, -0.1391,  0.1791],\n",
      "         [-0.1159, -0.1716,  0.2443,  ...,  0.9176, -0.9874,  0.0983],\n",
      "         [-0.0056, -0.0829, -0.4190,  ...,  0.9390, -0.7476,  0.7224],\n",
      "         ...,\n",
      "         [-0.8440, -0.4264, -0.3613,  ...,  0.8687, -0.2631,  0.2862],\n",
      "         [-0.7841, -0.0480, -0.4827,  ...,  0.4807, -0.1298,  0.2611],\n",
      "         [-0.6237, -0.0068,  0.0317,  ...,  0.3305,  0.0342,  0.0811]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "Shape of the final Output torch.Size([1, 13, 768])\n"
     ]
    }
   ],
   "source": [
    "#Check configurations of model\n",
    "# Check if the model supports attention weights\n",
    "config = BertConfig.from_pretrained(machine_T )\n",
    "if config.output_attentions:\n",
    "    # Extract hidden states\n",
    "    hidden_states = outputs.last_hidden_state\n",
    "\n",
    "    # Self-attention\n",
    "    self_attention_weights = outputs.attentions\n",
    "\n",
    "    # Print self-attention weights\n",
    "    print(\"Self-attention weights:\")\n",
    "    for layer, attn_weights in enumerate(self_attention_weights):\n",
    "        print(f\"Layer {layer+1}: {attn_weights.mean(dim=1)}\")\n",
    "\n",
    "# Decoding input text\n",
    "decoded_output = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "print(\"Decoded output from positional encoder:\", decoded_output)\n",
    "\n",
    "# Get the model's final output\n",
    "final_output = outputs[0]\n",
    "\n",
    "# Print the final output\n",
    "print(\"Final output:\", final_output)\n",
    "\n",
    "print(\"Shape of the final Output\",final_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20b3d57-ab2e-455c-9be4-b4c5154cde14",
   "metadata": {},
   "source": [
    "## Example Language Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4aea650d-9b5e-459d-9e6b-41df3c319d80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\sajag177350\\appdata\\local\\anaconda3\\lib\\site-packages (2.7.1+cpu)\n",
      "Requirement already satisfied: filelock in c:\\users\\sajag177350\\appdata\\local\\anaconda3\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\sajag177350\\appdata\\local\\anaconda3\\lib\\site-packages (from torch) (4.14.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\sajag177350\\appdata\\local\\anaconda3\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\sajag177350\\appdata\\local\\anaconda3\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\sajag177350\\appdata\\local\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\sajag177350\\appdata\\local\\anaconda3\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\sajag177350\\appdata\\local\\anaconda3\\lib\\site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\sajag177350\\appdata\\local\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\sajag177350\\appdata\\local\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac46fc93-a9a6-449c-a25e-4f6a9f1e13c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in c:\\users\\sajag177350\\appdata\\local\\anaconda3\\lib\\site-packages (0.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b78f9ec7-5863-4912-8eba-1040e2912e0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f5415529a1f49ecbab19142bf74948d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/306M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter text for transalation =  I am fine and happy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input= I am fine and happy\n",
      "Translated Output (Hindi):\n",
      "मैं ठीक हूँ और खुश हूँ\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"HF_HUB_DISABLE_SYMLINKS_WARNING\"] = \"1\"\n",
    "import torch\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "# Load pre-trained MarianMT model and tokenizer for English to Hindi translation\n",
    "model_name = 'Helsinki-NLP/opus-mt-en-hi'\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "hindi_tran = MarianMTModel.from_pretrained(model_name)\n",
    "\n",
    "# Input text\n",
    "input_text = input(\"Enter text for transalation = \")\n",
    "print(\"Input=\",input_text)\n",
    "\n",
    "# Tokenize input text\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# Perform translation\n",
    "translated_output = hindi_tran.generate(**input_ids)\n",
    "\n",
    "# Decode the translated output\n",
    "translated_text = tokenizer.decode(translated_output[0], skip_special_tokens=True)\n",
    "\n",
    "# Print the translated output\n",
    "print(\"Translated Output (Hindi):\")\n",
    "print(translated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df1bb6f8-e9ad-4fef-b3e7-badcf39c5a51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6ebec8d9ed84469ad05d8052628c031",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c41dc4d251064511b3ab73245d5b9d83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/298M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b5067543bf344f994a7d5c80eebaf92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/293 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de384484a2a7475688d0fd6e0a4b0cd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/298M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the Text:  I am Sajag\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded text in German: Ich bin Sajag.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"HF_HUB_DISABLE_SYMLINKS_WARNING\"] = \"1\"\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "# Load the pre-trained translation model and tokenizer for English to German\n",
    "model_name = \"Helsinki-NLP/opus-mt-en-de\"\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "\n",
    "# Input text in English\n",
    "input_text = input(\"Enter the Text: \")\n",
    "\n",
    "# Tokenize the input text\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# Perform translation\n",
    "outputs = model.generate(**inputs)\n",
    "\n",
    "# Decode the translated text\n",
    "decoded_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Print the decoded text\n",
    "print(\"Decoded text in German:\", decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9bc79d-09d4-41bf-8798-584a9342b8fd",
   "metadata": {},
   "source": [
    "## Hindi translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9b0c2b21-5f32-4250-b94c-83b0fbfd81f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hindi Translation: हैलो, तुम कैसे हो?\n"
     ]
    }
   ],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "import torch\n",
    "\n",
    "# Define the model and tokenizer for English to Hindi\n",
    "model_name_en_to_hi = 'Helsinki-NLP/opus-mt-en-hi'\n",
    "model_en_to_hi = MarianMTModel.from_pretrained(model_name_en_to_hi)\n",
    "tokenizer_en_to_hi = MarianTokenizer.from_pretrained(model_name_en_to_hi)\n",
    "\n",
    "def translate_en_to_hi(text):\n",
    "    inputs = tokenizer_en_to_hi(text, return_tensors='pt', padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        translated_tokens = model_en_to_hi.generate(**inputs)\n",
    "    return tokenizer_en_to_hi.batch_decode(translated_tokens, skip_special_tokens=True)[0]\n",
    "\n",
    "english_text = \"Hello, how are you?\"\n",
    "hindi_text = translate_en_to_hi(english_text)\n",
    "print(f'Hindi Translation: {hindi_text}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a4dbcaa3-bc2a-4e81-a281-c0d44f130325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wordcloud\n",
      "  Downloading wordcloud-1.9.4-cp312-cp312-win_amd64.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: pip in c:\\users\\sajag177350\\appdata\\local\\anaconda3\\lib\\site-packages (24.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement install (from versions: none)\n",
      "ERROR: No matching distribution found for install\n"
     ]
    }
   ],
   "source": [
    "pip install wordcloud --trusted-host pypi.org --trusted-host files.pythonhosted.org pip install googletrans==4.0.0-rc1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec8f795-d001-4243-bbf7-7a5a198ce7d8",
   "metadata": {},
   "source": [
    "## Sentiment Analysis\n",
    "\n",
    "-  We are using distilbert-base-uncased-finetuned-sst-2-english model, which is fine-tuned for sentiment analysis on the Stanford Sentiment Treebank (SST-2) dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c0c99e0d-9e1b-48fd-9e10-7723e0e0d7ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the text you want to classify:  I am happy and sad as well. I don't know what to be positive or negative\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted label: Positive\n",
      "Probabilities: [0.326862 0.673138]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"HF_HUB_DISABLE_SYMLINKS_WARNING\"] = \"1\"\n",
    "\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import numpy as np\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForSequenceClassification.from_pretrained(model_name)\n",
    "model.eval()\n",
    "\n",
    "# Define labels\n",
    "labels = ['Negative', 'Positive']\n",
    "\n",
    "# Function to perform text classification\n",
    "def classify_text(text):\n",
    "    # Tokenize input text\n",
    "    # Padding - You apply uniform length for tokens so that you get good results. Similar to images where you make padding to reduce loss\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    # Classify text using BERT model\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    # Get predicted probabilities\n",
    "    probabilities = torch.softmax(outputs.logits, dim=1).squeeze().numpy()\n",
    "    # Get predicted label index\n",
    "    predicted_label_index = np.argmax(probabilities)\n",
    "    # Get predicted label\n",
    "    predicted_label = labels[predicted_label_index]\n",
    "    return predicted_label, probabilities\n",
    "\n",
    "# User input for text\n",
    "user_text = input(\"Enter the text you want to classify: \")\n",
    "\n",
    "# Perform text classification\n",
    "predicted_label, probabilities = classify_text(user_text)\n",
    "\n",
    "# Print results\n",
    "print(\"Predicted label:\", predicted_label)\n",
    "print(\"Probabilities:\", probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6862ff99-6841-46fe-a3d9-5cf26b388748",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
